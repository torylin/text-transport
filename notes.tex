\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage[parfill]{parskip}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{geometry}
\geometry{margin=1in}
\newcommand{\vl}[1]{\textcolor{orange}{[VL: #1]}}
\newcommand{\eli}[1]{\textcolor{orange}{[Eli: #1]}}
\allowdisplaybreaks

\title{Causal Effect Estimation with Text}
\author{}
\date{}

\begin{document}
	
\maketitle

\section{Problem setting}

Consider a collection of texts (e.g., documents, sentences, utterances) $\mathcal{X}$, with individual texts $X_i \in \mathcal{X}$.
\begin{itemize}
    \item Let $X_i = \{g(X_i), h(X_i)\}$, where $g(X_i)$ is the text attribute of interest (i.e., the \textit{treatment}) and $h(X_i)$ denotes all other properties of the text $X_i$. 
    \item Let $Y_i(X_i) = Y_i(g(X_i), h(X_i))$ denote the potential \textit{response} or \textit{outcome} of respondent $i$ after reading text $X_i$.
    \item For simplicity, assume
    \begin{itemize}
        \item $g(X_i) \in \{0, 1\}$ $\forall$ $X_i \in \mathcal{X}$.
        \item Each respondent reads only one text. That is, respondent $i$ reads only $X_i$.
    \end{itemize}
\end{itemize}

We are interested in knowing the effect of the text attribute $g(X)$ on the response $Y$. Using the standard causal notation, the \textbf{estimand} $\tau^*$ for the effect of $g(X)$ on $Y$ is given by
\begin{align*}
    \tau^* &= \mathbb{E}_X[Y_i(g(X_i)=1)] - \mathbb{E}_X[Y_i(g(X_i)=0)] \\
    &= \boxed{\sum_{b \in \mathcal{B}} \Big[ \mathbb{E}[Y_i(g(X_i)=1, h(X_i)=b)] - \mathbb{E}[Y_i(g(X_i)=0, h(X_i)=b] \Big]P(h(X_i)=b)}
\end{align*}

Using stochastic notation, we can write $\tau^*$ more simply.
\begin{itemize}
    \item Let $P_{1 g,h}(X)$ denote a distribution such that $g(X)=1$ and $h(X) \sim P^*$, where $P^*$ is some arbitrary probability distribution.
    \item Let $P_{0 g,h}(X)$ denote a distribution such that $g(X)=0$ and $h(X) \sim P^*$.
    \item Let the quantity $\mu(P)$ be defined as follows:
    \begin{align*}
        \mu(P) &= E_{X_i \sim P_{g,h}}[Y_i(X_i)] \\
        &= \frac{1}{N} \sum_{i=1}^N Y_i(X_i)P_{g,h}(X_i)
    \end{align*}
\end{itemize}

This allows us to express $\tau^*$ as
\begin{equation*}
    \boxed{\tau^* = \mu(P_1)-\mu(P_0)}
\end{equation*}

\section{Estimator}

\subsection{Data setting}

Suppose we have some data collected from a randomized trial, in which subjects $i \in [N]$ are shown various texts randomized over $g(X)$ and $h(X)$. These texts are constructed from individual components in a generative way. 
\begin{itemize}
    \item That is, to construct a text where $g(X)=1$, the text may include one of several selected sentences that are chosen to correspond to $g(X)=1$.
    \item Likewise, for various attributes comprising $h(X)=(h_1(X), h_2(X), h_3(X), \dots)$, there may be several candidate texts that correspond to $h_1(X)=1$, several candidate texts that correspond to $h_2(X)=0$, and so on.
\end{itemize}

While we are guaranteed to be able to obtain an unbiased effect estimate for $g(X)$ from this trial (e.g., using the plug-in estimator), the estimate corresponds only to the effect of $g(X)$ \textit{in this specific text setting}.\footnote{``Vignette experiments only provide a local treatment effect. Extrapolating from this local treatment effect to the population of relevant texts requires the strong assumption that the many possible ways of delivering
the latent treatment do not interact with the latent treatment, causing its effect on the outcome to change.'' \cite{fong2021causal}} The type of text setting constructed in this trial is fairly artificial, and we believe that the effect may be different depending on the text setting. It is important, for example, to know if this effect still holds in natural text settings.

Therefore, we are interested in knowing the effect of $g(X)$ under text distributions that are different from the initial text distribution. Again, some notation:

\begin{itemize}
    \item Let $P^R_{g,h} = P^R$ denote the \textit{randomization distribution}, or the distribution of texts as constructed in the randomized trial.
    \item Let $P^T_{g,h} = P^T$ denote the \textit{target distribution}, or the text distribution of interest.
    \item Let $\mathbb{P}(A) = \int_A P(X) dx$.
    \item Using the Radon-Nikodym derivative for change of measure (see Section \ref{sec:change_of_measure}), we can write $P^T$ in terms of $P^R$ as
    \begin{equation*}
        P^T(X) = \frac{d\mathbb{P}^T}{d\mathbb{P}^R}(X)P^R(X)
    \end{equation*}
    \begin{itemize}
        \item We require \href{https://en.wikipedia.org/wiki/Absolute_continuity#Absolute_continuity_of_measures}{absolute continuity} of $\mathbb{P}^T$ with respect to  $\mathbb{P}^R$, such that $P^R(X) = 0 \Rightarrow P^R(X) = 0$.
    \end{itemize}
    \item Now, let $(X_i,Y_i(X_i))_{i=1}^n \sim P^R$ correspond to \textit{observed} pairs of features and outcomes sampled from the randomization distribution $P^R$.
\end{itemize}

\subsection{Horvitz-Thompson estimator}

To define our effect estimate in terms of our target distribution $P^T$, we propose using the ratio between $P^T$ and $P^R$ as importance weights in an Horvitz-Thompson estimator. Using the change of measure for $P^T(X)$ we defined previously, this gives us 
% \vl{Do we need to normalize by $\frac{1}{\sum_{i=1}^n \hat{P}^T(X_i)}$ in the first line?}
\begin{align*}
    \hat{\mu}(P^T) &= \frac{1}{n} \sum_{i=1}^n \frac{\hat{P}^T(X_i)}{P^R(X_i)}Y_i(X_i) \\
    &= \frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)\frac{P^R(X_i)}{P^R(X_i)} Y_i(X_i) \\
    &=\boxed{\frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)}
\end{align*}

We can show that $\hat{\mu}(P^T)$ is an unbiased estimator for $\mu(P^T)$:
\begin{align*}
    \mathbb{E}[\hat{\mu}(P)] &= \mathbb{E}_{X \sim P^R}[\hat{\mu}(P)] \\
    &= \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\right] \\
    &=\frac{1}{n}\sum_{i=1}^n \mathbb{E}_{X \sim P^R}\left[\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\right] \\
    &= \mathbb{E}_{X \sim P^R}\left[\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\right] \\
    &=\mathbb{E}_{X \sim P^T}[Y_i(X_i)] \\
    &= \mu(P^T)
\end{align*}

Finally, let $\mathcal{X}$ be the space of all texts, and consider the finite-sample setting where $Y_i$ is fixed (i.e., non-random) for all $i \in [n]$. Then the variance of the estimator is given by:
\begin{align*}
    \text{Var}[\hat{\mu}(P)|Y_i] &= \text{Var}\left[\frac{1}{n}\sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i) \right] \\
    &= \frac{1}{n^2}\sum_{i=1}^n\text{Var}\left[\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\right] \\
    &= \frac{1}{n^2}\sum_{i=1}^n\text{Cov}\left[\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i), \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\right] \\    
    &= \frac{1}{n^2}\sum_{i=1}^n\text{Cov}\left[\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x)Y_i(x) \mathbbm{1}\{X_i=x\}, \sum_{x' \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x')Y_i(x')\mathbbm{1}\{X_i=x'\}\right] \\
    &= \frac{1}{n^2}\sum_{i=1}^n\sum_{x \in \mathcal{X}}\sum_{x' \in \mathcal{X}} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x')Y_i(x)Y_i(x')\text{Cov}[\mathbbm{1}\{X_i=x\},\mathbbm{1}\{X_i=x'\}] \\
    &=\frac{1}{n^2}\sum_{i=1}^n\sum_{x \in \mathcal{X}}\sum_{x' \in \mathcal{X}} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x')Y_i(x)Y_i(x')(\mathbb{E}[\mathbbm{1}\{X_i=x\}\mathbbm{1}\{X_i=x'\}]-\mathbb{E}[\mathbbm{1}\{X_i=x\}]\mathbb{E}[\mathbbm{1}\{X_i=x'\}]) \\
    &=\frac{1}{n^2}\sum_{i=1}^n\sum_{x \in \mathcal{X}}\sum_{x' \in \mathcal{X}} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x')Y_i(x)Y_i(x')(P^R(x,x')-P^R(x)P^R(x'))
\end{align*}

If $x=x'$,
\begin{align*}
    \text{Var}[\hat{\mu}(P)|Y_i] &= \frac{1}{n^2}\sum_{i=1}^n\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}^2}{d \mathbb{P}^R}(x) Y_i^2(x)(P^R(x)-P^R(x)P^R(x)) \\
    &=\frac{1}{n^2}\sum_{i=1}^n\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}^2}{d \mathbb{P}^R}(x) Y_i^2(x)P^R(x)(1-P^R(x))
\end{align*}

If $x\neq x'$,
\begin{align*}
    \text{Var}[\hat{\mu}(P)|Y_i] &=\frac{1}{n^2}\sum_{i=1}^n\sum_{x \in \mathcal{X}}\sum_{\substack{x' \in \mathcal{X} \\ x' \neq x}} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x')Y_i(x)Y_i(x')(\underbrace{P^R(x,x')}_{0}-P^R(x)P^R(x')) \\
    &=-\frac{1}{n^2}\sum_{i=1}^n\sum_{x \in \mathcal{X}}\sum_{\substack{x' \in \mathcal{X} \\ x' \neq x}} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x')Y_i(x)Y_i(x')P^R(x)P^R(x')) \\
\end{align*}

Putting the two cases together,
\begin{align*}
    \text{Var}[\hat{\mu}(P)|Y_i] &= \frac{1}{n^2}\sum_{i=1}^n\Bigg(\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}^2}{d \mathbb{P}^R}(x) Y_i^2(x)P^R(x)(1-P^R(x) - \sum_{x \in \mathcal{X}}\sum_{\substack{x' \in \mathcal{X} \\ x' \neq x}} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x')Y_i(x)Y_i(x')P^R(x)P^R(x')\Bigg) \\
    &= \frac{1}{n^2}\sum_{i=1}^n\Bigg(\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}^2}{d \mathbb{P}^R}(x) Y_i^2(x)P^R(x) - \sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}^2}{d \mathbb{P}^R}(x) Y_i^2(x)P^R(x)^2 \\
    &- \sum_{x \in \mathcal{X}}\sum_{\substack{x' \in \mathcal{X} \\ x' \neq x}} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x')Y_i(x)Y_i(x')P^R(x)P^R(x')\Bigg) \\
    &= \frac{1}{n^2}\sum_{i=1}^n\Bigg(\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}^2}{d \mathbb{P}^R}(x) Y_i^2(x)P^R(x) - \Bigg(\underbrace{\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}^2}{d \mathbb{P}^R}(x) Y_i^2(x)P^R(x)}_{\hat{\mu}_i=\hat{\mathbb{E}}_{x \sim P^T}[Y_i(x)]=\hat{\mathbb{E}}_{x \sim P^R}[\frac{dP^T}{dP^R}(x)Y_i(x)]}\Bigg)^2\Bigg) \\
    &=\frac{1}{n^2}\sum_{i=1}^n\Bigg(\sum_{x \in \mathcal{X}}\Bigg(\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) Y_i(x)\Bigg)^2P^R(x) -\hat{\mu}_i^2\Bigg) \\
    &=\frac{1}{n^2}\sum_{i=1}^n\Bigg(\sum_{x \in \mathcal{X}}\Bigg(\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) Y_i(x) - \hat{\mu}_i \Bigg)^2P^R(x) + 2\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x)\hat{\mu}_i Y_i(x)P^R(x) - \sum_{x \in \mathcal{X}}\hat{\mu}_i^2P^R(x) - \hat{\mu}_i^2\Bigg) \\
    &=\frac{1}{n^2}\sum_{i=1}^n\Bigg(\sum_{x \in \mathcal{X}}\Bigg(\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) Y_i(x) - \hat{\mu}_i \Bigg)^2P^R(x) + \hat{\mu}_i\Bigg(2\underbrace{\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) Y_i(x)P^R(x)}_{\hat{\mu}_i} - \hat{\mu}_i\underbrace{\sum_{x \in \mathcal{X}}P^R(x)}_{1} - \hat{\mu}_i\Bigg)\Bigg) \\
    &=\frac{1}{n^2}\sum_{i=1}^n\Bigg(\sum_{x \in \mathcal{X}}\Bigg(\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) Y_i(x) - \hat{\mu}_i \Bigg)^2P^R(x) + \hat{\mu}_i(\underbrace{2\hat{\mu}_i-\hat{\mu}_i-\hat{\mu}_i}_{0})\Bigg) \\
    &=\frac{1}{n^2}\sum_{i=1}^n\sum_{x \in \mathcal{X}}\Bigg(\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) Y_i(x) - \hat{\mu}_i \Bigg)^2P^R(x)
\end{align*}

Then finally, letting $\hat{\mu}=\hat{\mathbb{E}}_{x \sim P^T}\left[\frac{1}{n}\sum_{i=1}^nY_i(x)\right]=\hat{\mathbb{E}}_{x \sim P^R}\left[\frac{1}{n}\sum_{i=1}^n\frac{dP^T}{dP^R}(x)Y_i(x)\right]$, we have
\begin{align*}
    \text{Var}[\hat{\mu}(P)] &= \mathbb{E}_Y[\text{Var}[\hat{\mu}(P)|Y_i]] \\
    &=\mathbb{E}_Y\left[\frac{1}{n^2}\sum_{i=1}^n\sum_{x \in \mathcal{X}}\Bigg(\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) Y_i(x) - \hat{\mu}_i \Bigg)^2P^R(x)\right] \\
    &=\frac{1}{n^2}\sum_{i=1}^n\sum_{x \in \mathcal{X}}\Bigg(\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x) Y_i(x) - \hat{\mu} \Bigg)^2P^R(x)
\end{align*}
% \begin{align*}
%     \text{Var}[\hat{\mu}(P)] &= \text{Var}\left[\frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\right] \\
%     &= \text{Cov}\left[\frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i), \frac{1}{n} \sum_{j=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j(X_j)\right] \\
%     &= \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \text{Cov}\left[\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i), \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j(X_j)\right] \\
%     &= \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \text{Cov}\left[\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\mathbbm{1}\{X_i = x\}, \sum_{x' \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j(X_j)\mathbbm{1}\{X_j=x'\}\right] \\
%     &= \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \text{Cov}\left[\int_x\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x)Y_i(x)\mathbbm{1}\{X_i = x\}d\mathbb{P}^R(x), \int_{x'}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x')Y_j(x')\mathbbm{1}\{X_j=x'\}d\mathbb{P}^R(x')\right] \\
%     &= \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n \int_x \int_{x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x') \text{Cov}[Y_i(x)\mathbbm{1}\{X_i=x\}, Y_j(x')\mathbbm{1}\{X_i=x'\}]d\mathbb{P}^R(x)d\mathbb{P}^R(x') \\
%     &= \frac{1}{n^2} \sum_{i,j \in [n]} \iint_{x, x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x') \text{Cov}[Y_i(x)\mathbbm{1}\{X_i=x\}, Y_j(x')\mathbbm{1}\{X_i=x'\}]d\mathbb{P}^R(x)d\mathbb{P}^R(x') \\
%     % &= \frac{1}{n^2} \sum_{i,j \in [n]} \iint_{x, x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x')Y_i(x)Y_j(x') (P^R(x,x') - P^R(x)P^R(x'))d\mathbb{P}^R(x)d\mathbb{P}^R(x') \\
%     &= \frac{1}{n^2} \sum_{i,j \in [n]} \iint_{x, x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x') \Big[\mathbb{E}[Y_i(x)Y_j(x')]P^R(x,x') - \mathbb{E}[Y_i(x)]\mathbb{E}[Y_j(x')]P^R(x)P^R(x')\Big] d\mathbb{P}^R(x)d\mathbb{P}^R(x')
% \end{align*}

% where the last equality follows from
% \begin{align*}
%     &\text{Cov}[Y_i(x)\mathbbm{1}\{X_i=x\}, Y_j(x')\mathbbm{1}\{X_i=x'\}] \\
%     % &=
%     % \mathbb{E}[(Y_i(x)\mathbbm{1}\{X_i=x\} - \mathbb{E}[Y_i(x)\mathbbm{1}\{X_i=x\}])(Y_j(x')\mathbbm{1}\{X_j=x'\} - \mathbb{E}[Y_j(x')\mathbbm{1}\{X_j=x'\}])] \\
%     % &= \mathbb{E}[(Y_i(x)\mathbbm{1}\{X_i=x\} - \mathbb{E}[Y_i(x)]\mathbb{E}[\mathbbm{1}\{X_i=x\}])(Y_j(x')\mathbbm{1}\{X_j=x'\} - \mathbb{E}[Y_j(x')]\mathbb{E}[\mathbbm{1}\{X_j=x'\}])] \\
%     % &= \mathbb{E}[(Y_i(x)\mathbbm{1}\{X_i=x\} - \mathbb{E}[Y_i(x)]P^R(x))(Y_j(x')\mathbbm{1}\{X_j=x'\} - \mathbb{E}[Y_j(x')]P^R(x'))] \\
%     &= \mathbb{E}[Y_i(x)\mathbbm{1}\{X_i=x\}Y_j(x')\mathbbm{1}\{X_j=x'\}] - \mathbb{E}[Y_i(x)\mathbbm{1}\{X_i=x\}]\mathbb{E}[Y_j(x')\mathbbm{1}\{X_j=x'\}] \\
%     &= \mathbb{E}[Y_i(x)Y_j(x')]\mathbb{E}[\mathbbm{1}\{X_i=x\}\mathbbm{1}\{X_j=x'\}]  - \mathbb{E}[Y_i(x)]\mathbb{E}[Y_j(x')]\mathbb{E}[\mathbbm{1}\{X_i=x\}]\mathbb{E}[\mathbbm{1}\{X_j=x'\}] \\
%     &=\mathbb{E}[Y_i(x)Y_j(x')]P^R(x,x') - \mathbb{E}[Y_i(x)]\mathbb{E}[Y_j(x')]P^R(x)P^R(x')
% \end{align*}

% Assuming independence of treatments, this further reduces to
% \begin{align*}
%     \text{Var}[\hat{\mu}(P)] &= \frac{1}{n^2} \sum_{i,j \in [n]} \iint_{x, x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x') \Big[\mathbb{E}[Y_i(x)]\mathbb{E}[Y_j(x')]P^R(x,x') - \mathbb{E}[Y_i(x)]\mathbb{E}[Y_j(x')]P^R(x)P^R(x')\Big] d\mathbb{P}^R(x)d\mathbb{P}^R(x') \\
%     &= \frac{1}{n^2} \sum_{i,j \in [n]} \iint_{x, x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x') \mathbb{E}[Y_i(x)]\mathbb{E}[Y_j(x')](P^R(x,x') - P^R(x)P^R(x')) d\mathbb{P}^R(x)d\mathbb{P}^R(x')
% \end{align*}

% \begin{align*}
%     \text{Var}[\hat{\mu}(P)] &= \text{Var}_X\left[\frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\right] \\
%     &= \text{Cov}_X\left[\frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i), \frac{1}{n} \sum_{j=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j(X_j)\right] \\
%     &= \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \text{Cov}_X\left[\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i), \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j(X_j)\right] \\
%     &= \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \text{Cov}_X\left[\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\mathbbm{1}\{X_i = x\}, \sum_{x' \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j(X_j)\mathbbm{1}\{X_j=x'\}\right] \\
%     &= \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \text{Cov}_X\left[\int_x\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x)Y_i(x)\mathbbm{1}\{X_i = x\}d\mathbb{P}^R(x), \int_{x'}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x')Y_j(x')\mathbbm{1}\{X_j=x'\}d\mathbb{P}^R(x')\right] \\
%     &= \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n \int_x \int_{x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x')Y_i(x)Y_j(x') \text{Cov}_X[\mathbbm{1}\{X_i=x\}, \mathbbm{1}\{X_i=x'\}]d\mathbb{P}^R(x)d\mathbb{P}^R(x') \\
%     &= \frac{1}{n^2} \sum_{i,j \in [n]} \iint_{x, x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x')Y_i(x)Y_j(x') \text{Cov}_X[\mathbbm{1}\{X_i=x\}, \mathbbm{1}\{X_i=x'\}]d\mathbb{P}^R(x)d\mathbb{P}^R(x') \\
%     &= \frac{1}{n^2} \sum_{i,j \in [n]} \iint_{x, x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(x')Y_i(x)Y_j(x') (P^R(x,x') - P^R(x)P^R(x'))d\mathbb{P}^R(x)d\mathbb{P}^R(x')
% \end{align*}
% \eli{I think this generally gets to the right place, but e.g. the inner term in some of those covariances should be $\int_x\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(x)Y_i(x)\mathbbm{1}\{X_i = x\}d\mathbb{P}^R(x)$, so that only $\mathbbm{1}\{X_i = x\}d\mathbb{P}^R(x)$ is random}
% where the last equality follows from
% \begin{align*}
%     \text{Cov}[\mathbbm{1}\{X_i=x\},\mathbbm{1}\{X_j=x'\}] &=\mathbb{E}_X[\mathbbm{1}\{X_i=x\}\mathbbm{1}\{X_j=x'\}] - \mathbb{E}_X[\mathbbm{1}\{X_i=x\}]\mathbb{E}_X[\mathbbm{1}\{X_j=x'\}] \\
%     &= P^R(x,x') - P^R(x)P^R(x')
% \end{align*}

With the central limit theorem (CLT), we establish asymptotic normality:
\begin{equation*}
    \frac{\hat{\mu}(P) - \mu(P)}{\sqrt{\text{Var}[\hat{\mu}(P)]}}\rightarrow N(0,1)
\end{equation*}

which we can use to estimate confidence intervals using the following unbiased variance estimate:

\begin{equation*}
    \widehat{\text{Var}}[\hat{\mu}(P)] = \frac{1}{n^2} \sum_{i \in [n]} \left(\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)-\hat{\mu_i}\right)^2
\end{equation*}

% $$\widehat{\text{Var}}[\hat{\mu}(P)] =\frac{1}{|D_1|(|D_1|-1)} \sum_{i \in D_1} \gamma_i^2(Y_i-\overline{Y}_{D_1})^2 + \frac{1}{|D_0|(|D_0|-1)} \sum_{j \in D_0} \gamma_j^2(Y_j-\overline{Y}_{D_0})^2$$

% \vl{QUESTION: Do we compute variance estimates separately for $\hat{\mu}(P_1)$ and $\hat{\mu}(P_1)$, then compute $$\widehat{\text{Var}}(\hat{\tau})=\frac{\widehat{\text{Var}}[\hat{\mu}(P_1)]}{n^2P(g(X)=1)^2} + \frac{\widehat{\text{Var}}[\hat{\mu}(P_0)]}{n^2P(g(X)=0)^2}$$ Or do we use this variance estimate over the whole sample to directly get $\widehat{\text{Var}}(\hat{\tau})$?
% ANSWER: In the finite-sample setting, adding the variance estimates is fine. In the non-finite-sample setting, the variance estimation is more involved.}

\section{Empirical estimation}

\subsection{Estimating $\hat{P}^T$ from the data}

\vl{QUESTION: Should we ever use the full randomization corpus (i.e., all possible 2- or 3-attribute permutations of candidate sentences from the HK study) as $P^R$, or do we always consider $P^R$ to be the sample (i.e., the data for which we have measured outcomes)? This is relevant because the distributions are not the same---the reason being that there are different numbers of candidate sentences for each text attribute, so a full set of permutations is not uniform over the confounding attributes. 
ANSWER: Instead of getting all possible permutations, get the full randomization distribution by just sampling a large number of texts according to the way Fong \& Grimmer sampled theirs. That is, first sample attributes, then sample sentences from those attributes.}

\vl{As a related question, should the distribution of number of sentences per text in $P^T$ follow sample $P^R$ or full-corpus $P^R$?}

Our estimator has one primary nuisance parameter, $\frac{\hat{d\mathbb{P}^T}(X)}{d\mathbb{P}^R(X)}$ (or $\hat{P}^T(X)$, depending on the version of the estimator we want to use). We propose several approaches for estimating this quantity.

\vl{QUESTION: Is sample splitting necessary when learning the nuisance parameter? For now, I have split off a very small sample (10\% of the data) for training the classifier.
ANSWER: Maybe?}

\subsubsection{Classification}
\label{sec:classification}

We notice that we can rewrite $\frac{d\mathbb{P}^T}{d\mathbb{P}^R}$. Let $C$ denote the distribution (or corpus) from which a text is drawn, where $C=T$ denotes that it is drawn from $P^T$ and $C=R$ denotes that it is drawn from $P^R$. Then
\begin{equation*}
    \begin{split}
        &\frac{d\mathbb{P}^T}{d\mathbb{P}^R}(X) = \frac{P(C=T|X)}{P(C=T)}\frac{P(C=R)}{P(C=R|X)}\\
        \Rightarrow &\frac{\hat{d\mathbb{P}^T}}{d\mathbb{P}^R}(X) = \frac{\hat{P}(C=T|X)}{P(C=T)}\frac{P(C=R)}{\hat{P}(C=R|X)}
    \end{split}
\end{equation*}

where estimation of $\hat{P}(C=T|X)$ and $\hat{P}(C=R|X)$ is straightforward---we can train a binary classifier $M_\theta: \mathcal{X} \rightarrow \{0,1\}$ to predict if a text $X$ came from $T$ or $R$---and $P(C=R)$ and $P(C=T)$ are their sample proportions over the entire body of text.

We have several options for our binary classifier:
\begin{itemize}
    \item A model that takes ``interpretable'' language features as input (e.g., bag-of-words, lexicon, SenteCon). We train a ``simple'' classifier over these features (e.g., logistic regression, SVM).
    \item A deep language model that takes raw text as input. We can either use a pre-trained model, or we can fine-tune the pre-trained model to differentiate between text examples from $P^T$ and text examples from $P^R$.
    \item \vl{Since texts from $P^R$ are artificial, the classification task might be very easy, so $\hat{P}(C=T|X)$ and $\hat{P}(C=R|X)$ may be very close to either 0 or 1. Instead of training a discriminative classifier, we could have two generative models, one for $P^R$ and one for $P^T$. Then given an $X$, which model is more likely? This could help with the overconfidence of the discriminative classifier.}
\end{itemize}

\subsubsection{Language modeling}
\label{sec:language_modeling}

Language models like transformers are capable of directly computing the probability of a sentence (see Section \ref{sec:huggingface_sentence_probs}), even if the model has never seen the sentence before. This probability is based on the text distribution used to train the model. Since the state-of-the-art large language models (LLMs) are pre-trained on extremely large corpora that approximate the entirety of the English language, we must further fine-tune before we can use them to compute sentence probabilities for $P^T$.

LLMs are often trained using masked language modeling (MLM), where for each training sentence, certain words are randomly masked, and the model must predict them. However, it is important to note that token and sentence probabilities from language models trained with MLM are not statistically or linguistically meaningful (this was actually \href{https://github.com/google-research/bert/issues/35}{confirmed} by Jacob Devlin when BERT was released). The token probabilities returned by the model are the probability of the masked token \textit{at a specific position}, \textit{given the rest of the unmasked tokens in the sentence}, rather than the probability of the token \textit{given the preceding tokens in the sentence}. 

Instead, to estimate meaningful sentence probabilities directly, \textit{autoregressive language models}, which are trained on causal language modeling\footnote{There's nothing causal about causal language modeling; it's just a naming convention.} (CLM)/text generation/next-word-prediction tasks, may be used. These include ``shallow'' language models like n-gram language models, as well as autoregressive LLMs like GPT-2, TransformerXL, and Reformer (a partial list of autoregressive LLMs implemented by HuggingFace can be found \href{https://web.archive.org/web/20230125132319/https://huggingface.co/docs/transformers/model_summary#autoregressive-models}{here}).

We can encourage an autoregressive language model $M_\theta$ to learn $P^T$ by fine-tuning it on text examples from $P^T$ using a CLM task. For a practical example of CLM, see \href{https://huggingface.co/docs/transformers/tasks/language_modeling}{this example from HuggingFace}.

% \begin{itemize}
%     \item We draw text examples from $P^T$. For each sentence, certain words are masked, and $M_\theta$ learns to predict them.
%     \item Causal language modeling (CLM)/text generation: Again, we drawn text samples from $P^T$. For each sentence, starting from the first word, $M_\theta$ learns to predict the next word.
%     \item Style transfer: Given a text example drawn from $P^R$, $M_\theta$ reproduces that same text example ``in the style of'' $P^T$. This task requires $M_\theta$ to be a sequence-to-sequence language model, like BART \cite{lewis2020bart}, and for best results, parallel text examples from $P^R$ and $P^T$ should be available.
% \end{itemize}

% For a practical example of MLM and CLM, see \href{https://huggingface.co/docs/transformers/tasks/language_modeling}{this example from HuggingFace}. For some ideas on how to fine-tune BART on a style transfer task, see \href{https://blog.fastforwardlabs.com/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html}{this blog post} and \cite{lai2021bart}.

\vl{Because texts from $P^R$ are very artificial (especially the sequences of sentences), it is possible that $P^T$ of those texts will be very low, perhaps more than they should be. $P^T$ of individual sentences may not be that low, but the overall construction will have low probability. Is this a problem? Maybe we should compute $P^T(\text{sentence 1})P^T(\text{sentence 2})P^T(\text{sentence 3})$ instead of $P^T(\text{sentence 1, sentence 2, sentence 3})$...}

\vl{QUESTION: If you fine-tune an LM, how do you guarantee that it's learned enough of $P^T$ and forgotten enough of its original training domain? Also, how do you guarantee that it moves away from $P^R$?
ANSWER: One potential solution is to use a shallow language model and train it \textit{only} on $P^T$, which ensures that the resulting sentence probabilities are defined only over $P^T$.}

\subsection{Estimating $\hat{\mu}(P)$ from the data}

We estimate the overall quantity of interest $\hat{\tau}$ through the following procedure. Suppose we have samples $(X_i,Y_i)_{i=1}^n \sim P^R$. In our experiments, these samples are from \cite{fong2021causal}. The authors conduct a randomized study on the effects of a text attribute $g(X)$ on an outcome $Y$ in a constructed text setting, which we consider to be distributed $P^R$.

\begin{enumerate}
    \item Sample examples from $P^T$. Train $M_\theta$ on these examples to be able to obtain $\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)$ or $\hat{P}^T(X_i)$ $\forall$ $i \in [n]$.
    \begin{itemize}
        \item[--] In our experiments, these examples from $P^T$ constitute 2-7 sentence chunks drawn from the U.S. Congressional speeches on the Hong Kong protests. We consider these to be the ``natural language counterparts'' of the examples from $P^R$.
    \end{itemize}
    \item (If using $P^T$) $P^R(X_i)$ should be known from the design of the randomized study. In the worst case, it can be computed empirically from sample proportions.
    \item Split $(X_i,Y_i)$ according to samples where $g(X_i)=1$ and where $g(X_i)=0$. Call these data splits $D_1$ and $D_0$, respectively.
    \item Compute
    \begin{equation*}
        \begin{split}
            \hat{\tau} &= \hat{\mu}(P_1) - \hat{\mu}(P_0) \\
            &= \frac{1}{nP(g(X)=1)} \sum_{j \in D_1} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j - \frac{1}{nP(g(X)=0)} \sum_{k \in D_0} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_k)Y_k \\
            &= \frac{1}{nP(g(X)=1)} \sum_{j \in D_1} \frac{\hat{P}(C=T|X_j)}{P(C=T)}\frac{P(C=R)}{\hat{P}(C=R|X_j)}Y_j - \frac{1}{nP(g(X)=0)} \sum_{k \in D_0} \frac{\hat{P}(C=T|X_k)}{P(C=T)}\frac{P(C=R)}{\hat{P}(C=R|X_k)}Y_k
        \end{split}
    \end{equation*}
    or alternatively (to ensure that the absolute continuity condition holds, we restrict $P^T$ with respect to the support of the randomization distribution, such that $P^T(X) = \frac{P^T(\tilde{X})\mathbbm{1}\{\tilde{X} \in C^R\}}{\sum_{i=1}^n P^T(X_i)}$)
    \begin{equation*}
        \begin{split}
            \hat{\tau} &= \hat{\mu}(P_1) - \hat{\mu}(P_0) \\
            &= \frac{1}{n P(g(X)=1)} \sum_{j \in D_1} \frac{\hat{P}^T(X_j)}{P^R(X_j)\sum_{i=1}^n \hat{P}^T(X_i)}Y_j - \frac{1}{nP(g(X)=0)} \sum_{k \in D_0} \frac{\hat{P}^T(X_k)}{P^R(X_k)\sum_{i=1}^n \hat{P}^T(X_i)}Y_k
        \end{split}
    \end{equation*}
    % \vl{Is the normalization just to ensure that $\hat{P}^T$ sums to 1 over $X_i \sim P^R$?}
    
    Note that $P(g(X)=1)$ and $P(g(X)=0)$ are defined over the entire randomization corpus (i.e., every possible permutation of the statements/linguistic attributes in the HK study), prior to sampling. In practical terms, this means they are computed from the  proportions of the randomization corpus, rather than the  proportions of the sampled data (i.e., the data for which we have measured outcomes).
    \item In practice, to maintain the stability of the weights (which can be very small), we use the \href{https://cran.r-project.org/web/packages/sampling/vignettes/HT_Hajek_estimators.pdf}{Hajek estimator} instead of the Horvitz-Thompson estimator. With the Hajek estimator, the importance weights are normalized by the \textit{average} importance weight, which yields the following estimators:
    \begin{equation*}
        \hat{\tau} = \frac{1}{|D_1|} \sum_{j \in D_1} \hat{\gamma}_j Y_j - \frac{1}{|D_0|} \sum_{k \in D_0} \hat{\gamma}_k Y_k
    \end{equation*}
    
    % \begin{equation*}
    %     \widehat{\text{Var}}[\hat{\mu}(P_1)] = \frac{1}{|D_1|^2} \sum_{i,j \in D_1} \hat{\gamma}_i \hat{\gamma}_j (Y_i-\bar{Y}_{D_1})(Y_j-\bar{Y}_{D_1}) \frac{P^R(X_i,X_j)-P^R(X_i)P^R(X_j)}{P^R(X_i,X_j)}
    % \end{equation*}

    \begin{equation*}
        \widehat{\text{Var}}[\hat{\mu}(P_1)] = \frac{1}{|D_1|^2} \sum_{i \in D_1} (\hat{\gamma}_i Y_i-\hat{\mu}(P_1))^2
    \end{equation*}
    
    % \begin{equation*}
    %     \widehat{\text{Var}}[\hat{\mu}(P_0)] = \frac{1}{|D_0|^2} \sum_{i,j \in D_0} \hat{\gamma}_i \hat{\gamma}_j (Y_i-\bar{Y}_{D_0})(Y_j-\bar{Y}_{D_0}) \frac{P^R(X_i,X_j)-P^R(X_i)P^R(X_j)}{P^R(X_i,X_j)}
    % \end{equation*}

    \begin{equation*}
        \widehat{\text{Var}}[\hat{\mu}(P_0)] = \frac{1}{|D_0|^2} \sum_{i \in D_0} (\hat{\gamma}_i Y_i-\hat{\mu}(P_0))^2
    \end{equation*}

    \vl{Not totally sure about this, but I thought we would have to subtract out the mean $Y$ so that the variance doesn't blow up with large $Y$?}
    
    \begin{equation*}
        \widehat{\text{Var}}[\hat{\tau}] = \widehat{\text{Var}}[\hat{\mu}(P_1)] + \widehat{\text{Var}}[\hat{\mu}(P_0)]
    \end{equation*}
    
    where for the classification approach,
    \begin{equation*}
        \hat{\gamma}_j = \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j) \Bigg/ \left(\frac{1}{|D|} \sum_{j \in D} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)\right)
    \end{equation*}
    and for the language modeling approach,
    \begin{equation*}
        \hat{\gamma}_j = \frac{\hat{P}^T(X_j)}{P^R(X_j)\sum_{i=1}^n \hat{P}^T(X_i)} \Bigg/ \left(\frac{1}{|D|} \sum_{j \in D} \frac{\hat{P}^T(X_j)}{P^R(X_j)\sum_{i=1}^n \hat{P}^T(X_i)} \right)
    \end{equation*}
\end{enumerate}

% \vl{Is there any reason Fong \& Grimmer used linear regression to estimate the effects instead of difference-in-means? I also implemented the option of LR on weight-adjusted $Y_i$s, but I've left the results out of the document (they do look different though).}

\subsection{``Natural'' vs. ``isolated'' effects}

We define a \textit{natural} effect to refer to the effect of a text attribute $g(X)$ \textit{as it typically co-occurs with other text attributes}. For instance, if we are interested in the text attribute \textit{positive emotion}, we know that a text that contains positive emotion may frequently also contain attributes like \textit{optimism} or \textit{friendliness}. Then the natural effect of \textit{positive emotion} is the collective effect of positive emotion \textit{and} its typical co-ocurring traits like optimism and friendliness.

An \textit{isolated} effect, on the other hand, is the effect of the text attribute $g(X)$ \textit{separate from all other text attributes}. Again taking positive emotion as $g(X)$, the isolated effect of $g(X)$ would be the effect of positive emotion only, if it could be separated from its co-occurring attributes. While it is difficult to think of the isolated effect in interpretable terms---what does it mean for a text attribute to exist in isolation?---it is something that can be computed. Randomized experiments with text, for example, can tell us the isolated effect of $g(X)$.

In general, for the isolated effect of $g(X)$ to be computed from a text distribution $P$, the distribution must be uniform over all other text attributes $h(X)$. This means that it is possible to obtain an isolated effect from our target distribution $P^T$ as well---but $P^T$ must be sampled in a specific way, such that uniformity is enforced over all $h(X)$. If $P^T$ is sampled in a ``natural'' way, then the effect we obtain is instead the natural effect. In our current experiments, we report natural effects.

\section{Experiments}

\subsection{Data}

\subsubsection{Randomization corpus: Hong Kong protests experimental study}

Our randomization corpus $P^R$ is taken from \cite{fong2021causal}, where texts are randomly constructed over several binary attributes and assigned to readers. For each text, 2-3 attributes are chosen out of a pool of 7. In this data, the measured variables comprise the following:
\begin{itemize}
    \item \textit{Response}: The reader's response to the question: ``On a scale from 0 to 100... how much do you agree with the following statement: The United States government should support the protesters in Hong Kong.''
    \item \textit{Number of attributes}: Either 2 or 3. The number of attributes included in the text.
    \item \textit{Commitment}: Whether the following is mentioned: the commitment the United States made to Hong Kong through the Hong Kong Policy Act of 1992.
    \item \textit{Bravery}: Whether the following is mentioned: the protesters' bravery in the face of physical harm.
    \item \textit{Mistreatment}: Whether the following is mentioned: China's history of mistreatment of its citizens.
    \item \textit{Flags}: Whether the following is mentioned: that the protestors were waving the American flag as a symbol of freedom and American values.
    \item \textit{Threat}: Whether the following is mentioned: the threat of future aggression from China to America or American allies.
    \item \textit{Economy}: Whether the following is mentioned: the historical autonomy of Hong Kong from China, including its economy and government.
    \item \textit{Violation}: Whether the following is mentioned: that China's actions violate the Sino-British Joint Declaration, which guarantees the autonomy of the economy and government of Hong Kong.
\end{itemize}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Treatment & $\hat{\tau}$ & 95\% CI \\
        \midrule
        Commitment & \textbf{\textcolor{green!50!black}{2.68$^*$}} & [0.27, 5.09] \\
        Bravery & \textcolor{green!50!black}{1.85} & [-0.85, 4.55] \\
        Mistreatment & \textcolor{green!50!black}{0.14} & [-2.58, 2.86] \\
        Flags & \textcolor{red!80!black}{-2.12} & [-4.84, 0.60] \\
        Threat & \textcolor{red!80!black}{-2.07} & [-4.74, 0.60]  \\
        Economy & \textcolor{red!80!black}{-0.94} & [-3.59, 1.71] \\
        Violation & \textcolor{green!50!black}{0.75} & [-1.95, 3.45] \\
        \bottomrule
    \end{tabular}
    \caption{For comparison, treatment effect estimates from the original Fong \& Grimmer study \cite{fong2021causal} (assuming normal confidence intervals).}
    \label{tab:results_baseline}
\end{table}

\subsubsection{Target corpus: U.S. Congressional speeches on Hong Kong protests}

Our target corpus $P^T$ is derived from the U.S. Congressional speeches on the Hong Kong protests, which \cite{fong2021causal} used as the basis for their constructed texts. This corpus comprises 20 speeches delivered during Congressional meetings between June and November 2019. We split each speech into passages of 2-7 sentences, following the sentence probabilities of the randomization corpus. We consider these passages to be the natural language analogs of the constructed texts in \cite{fong2021causal}.

\subsubsection{Amazon music reviews}

This dataset from \cite{przyant2021causal} contains Amazon reviews of music purchases, with a synthetic outcome $Y$ that is a function of (1) the rating of the review and (2) the media format of the music (CD or digital). This dataset can be considered ``randomized,'' which makes it suitable for use as $P^R$; however, by splitting the dataset according to media format, we can also treat one split as $P^R$ and the other as $P^T$. Then, since both splits are randomized, an effect transported from $P^R$ to $P^T$ can be validated against an effect estimated directly from $P^T$. We parametrize the text to obtain interpretable $g(X), h(X)$ using the LIWC and Empath lexicons.

\subsubsection{Amazon reviews}

This dataset contains Amazon reviews of musical instruments and office products, with a synthetic label (i.e., outcome $Y$) that is a noisy function of the text embeddings (from pre-trained DistilBERT) and LIWC categories. Specifically, we combine helpfulness predictions from a linear model fit on the top 10 most predictive LIWC categories and helpfulness predictions from fine-tuned DistilBERT (plus Gaussian noise). We generate three variants of the synthetic label:
\begin{itemize}
    \item \textbf{Direct combination}: The lexical component and the language model component are combined through an arbitrary ratio (in our case, $\text{lexical} + 0.5*\text{LM}$), then noised.
    \item \textbf{Regression combination}: An additional linear model is trained over the lexical and language model predictions, with helpfulness as the label. The predictions from this additional linear model are then noised.
    \item \textbf{Interaction}: A linear model is trained over all LIWC categories, the language model predictions, and the interactions between the LIWC categories and the language model predictions. The predictions from this interaction model are then noised.
\end{itemize}

As with the Amazon music reviews dataset, this dataset can be used for validation. $P^R$ and $P^T$ are obtained by splitting on the product category of the review. Again, we parameterize the text to obtain interpretable $g(X), h(X)$ using LIWC and Empath.

\subsubsection{A ``contrived'' setting}
\label{sec:contrived_data}

To ensure that our method works in the most controlled circumstances, we construct a very simple setting. Again using Amazon reviews of musical instruments and office products, we synthesize a label that is a noisy function (i.e., linear model prediction plus Gaussian noise) of the top 10 most predictive LIWC categories \textit{only}. Next, we split the reviews into $P^R$ and $P^T$ by fitting a classifier of music vs. text reviews, again as only a function of the same 10 LIWC categories. We sample $P^R$ and $P^T$ using the predicted probabilities of the classifier. Finally, we retain these predicted probabilities to use as $P(C=T|X)$ and $P(C=R|X)$ in our imporance weights.

Importantly, this construction ensures that
\begin{itemize}
    \item Both our label and our $P^R$/$P^T$ split are dependent on only the 10 lexical features (and, of course, the Gaussian noise).
    \item Our importance weights are ``known.'' 
\end{itemize}

Therefore, this simple synthetic dataset allows us to test whether our method works in its most basic, controlled setting where all confounding is accounted for and importance weights are known to be correct. If the method works (i.e., we can validate that $\hat{\mu}_{R \rightarrow T} \approx \hat{\mu}_T$) under these conditions, then we know that in other settings where the effect is not validated, the source of error is due to estimation or engineering rather than problems with the method itself.

\subsubsection{EmoBank}
\label{sec:emobank_data_description}

This dataset from \cite{buechel2017emobank} contains English sentences that have been annotated for valence (negative vs. positive), arousal (calm vs. excited), and dominance (being controlled vs. in control), both with respect to the \textit{writer's intent} and the \textit{reader's perception}. Because these annotations are crowdsourced, the dataset can be considered randomized (as annotators are randomly assigned to read and react to texts via the crowdsourcing platform).

We split the dataset according to the \textit{writer's} intended valence. For one split, which we consider $P^R$, we sample rows with high writer-intended valence (i.e., $>3$) with high probability and sample rows with low writer-intended valence (i.e., $<3$) with low probability. For the other split, which we consider $P^T$, we do the opposite. We then use the \textit{reader's} perceived valence as the outcome $Y$.

Because both $P^R$ and $P^T$ are randomized, we can use them for validation. Again, we parameterize the text to obtain interpretable $g(X), h(X)$ using LIWC and Empath.

\subsection{Results}

\subsubsection{Hong Kong: Classification approach}

We report our treatment effect estimates using the classification approach described in Section \ref{sec:classification}. For each treatment (i.e., text attribute), we estimate its effect on the \textit{response}. In Table \ref{tab:results_clf_mpnet}, the classifier is a logistic regression model, and the language representations are embeddings from the pre-trained sentence transformer MPNet \cite{song2020mpnet}. HuggingFace's sentence transformers are designed to be particularly suitable for generating sentence embeddings for a variety of tasks. In Table \ref{tab:results_clf_mpnet_marginal}, the classifier and language representations remain the same, but text probabilities are computed as a marginal product of the individual sentence probabilities. Variance is computed as a sum of $\widehat{\text{Var}}[\hat{\mu}(P_1)]$ and $\widehat{\text{Var}}[\hat{\mu}(P_0)]$.

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
        \midrule
        Commitment & 72.087 & 68.743 & \textcolor{green!50!black}{3.343} & [-3.973, 10.660] \\
        Bravery & 74.134 & 69.604 & \textcolor{green!50!black}{4.529} & [-6.178, 15.237] \\
        Mistreatment & 69.844 & 70.972 & \textcolor{red!80!black}{-1.127} & [-8.310, 6.056] \\
        Flags & 67.943 & 71.138 & \textcolor{red!80!black}{-3.195} & [-10.482, 4.092] \\
        Threat & 69.034 & 71.32 & \textcolor{red!80!black}{-2.285} & [-10.807, 6.237]  \\
        Economy & 70.254 & 70.857 & \textcolor{red!80!black}{-0.603} & [-9.482, 8.276] \\
        Violation & 70.729 & 70.731 & \textcolor{red!80!black}{-0.001} & [-8.301, 8.299] \\
        \bottomrule
    \end{tabular}
    \caption{(HK) Treatment effect estimates using the classification approach, where the classifier is a logistic regression model learned over embeddings from pre-trained MPNet.}
    \label{tab:results_clf_mpnet}
\end{table}

% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{c|cccc}
%         \toprule
%         Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
%         \midrule
%         Commitment & 74.701 & 70.379 & \textcolor{green!50!black}{4.322} & [-1.304, 9.948] \\
%         Bravery & 77.474 & 71.958 & \textcolor{green!50!black}{5.516} & [-1.190, 12.221] \\
%         Mistreatment & 69.899 & 73.844 & \textcolor{red!80!black}{-3.945} & [-9.782, 1.891] \\
%         Flags & 70.762 & 74.209 & \textcolor{red!80!black}{-3.447} & [-10.955, 4.061] \\
%         Threat & 75.492 & 72.778 & \textcolor{green!50!black}{2.714} & [-3.579, 9.008] \\
%         Economy & 75.116 & 73.335 & \textcolor{green!50!black}{1.781} & [-4.563, 8.126] \\
%         Violation & 69.977 & 73.585 & \textcolor{red!80!black}{-3.608} & [-11.861, 4.645] \\
%         \bottomrule
%     \end{tabular}
%     \caption{Treatment effect estimates using the classification approach, where the classifier is a logistic regression model learned over embeddings from pre-trained MPNet. Text probabilities are computed from marginal sentence probabilities.}
%     \label{tab:results_clf_mpnet_marginal}
% \end{table}

\subsubsection{Hong Kong: Language modeling approach}

\vl{Probably the biggest issue we have is the lack of data---it turns out that the natural text of the HK speeches is not a very large corpus. While the classifier doesn't require much training data because the task is fairly easy, it's unclear if properly fine-tuning an LLM requires more.
\newline
\newline
On the other hand, if we're interested in a ``real-world'' target setting without the constraints of it having to be Congressional speeches or about HK, we could just use a pre-trained LLM.}

We report our treatment effect estimates using the language modeling approach described in Section \ref{sec:language_modeling}. Again, for each treatment (i.e., text attribute), we estimate its effect on the \textit{response}. In Table \ref{tab:results_clm_gpt2}, due to the small size of the target corpus, we use pre-trained GPT-2 \cite{radford2019language} to compute the text probabilities under $P^T$. This means that our target distribution may now be viewed as something akin to the distribution of \textit{all} text, given the extremely large training corpus of GPT-2. In Table \ref{tab:results_clm_gpt2_finetuned}, we use GPT-2 fine-tuned with a CLM task on a small number of held-out samples (approximately 30) from $P^T$. In Table \ref{tab:results_clm_gpt2_marginal}, we use pre-trained GPT-2 but compute marginal sentence probabilities.

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
        \midrule
        Commitment & 71.92 & 68.363 & \textcolor{green!50!black}{3.557} & [-0.211, 8.268] \\
        Bravery & 72.785 & 69.571 & \textcolor{green!50!black}{3.214} & [-3.030, 7.762]  \\
        Mistreatment & 69.633 & 70.303 & \textcolor{red!80!black}{-0.670} & [-6.329, 3.846]  \\
        Flags & 69.022 & 70.422 & \textcolor{red!80!black}{-1.400} & [-6.586, 3.781] \\
        Threat & 68.908 & 70.505 & \textcolor{red!80!black}{-1.597} & [-5.727, 4.312] \\
        Economy & 69.971 & 70.235 & \textcolor{red!80!black}{-0.264} & [-4.718, 6.061] \\
        Violation & 71.025 & 70.013 & \textcolor{green!50!black}{1.011} & [-4.911, 5.246] \\
        \bottomrule
    \end{tabular}
    \caption{(HK) Treatment effect estimates using the language modeling approach, where text probabilities under $P^T$ are estimated from pre-trained GPT-2.}
    \label{tab:results_clm_gpt2}
\end{table}


\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
        \midrule
        Commitment & 72.046 & 68.347 & \textcolor{green!50!black}{3.699} & [-0.321, 7.719] \\
        Bravery & 73.1 & 69.598 & \textcolor{green!50!black}{3.503} & [-1.729, 8.734] \\
        Mistreatment & 69.788 & 70.341 & \textcolor{red!80!black}{-0.553} & [-5.472, 4.368] \\
        Flags & 69.122 & 70.463 & \textcolor{red!80!black}{-1.341} & [-6.382, 3.702] \\
        Threat & 68.92 & 70.565 & \textcolor{red!80!black}{-1.645} & [-6.438, 3.148] \\
        Economy & 70.158 & 70.269 & \textcolor{red!80!black}{-0.112} & [-5.210, 4.989] \\
        Violation & 71.11 & 70.077 & \textcolor{green!50!black}{1.033} & [-3.913, 5.979] \\
        \bottomrule
    \end{tabular}
    \caption{(HK) Treatment effect estimates using the language modeling approach, where text probabilities under $P^T$ are estimated from GPT-2 fine-tuned with a CLM task on a small number of held-out samples from $P^T$.}
    \label{tab:results_clm_gpt2_finetuned}
\end{table}

% \begin{table}[!ht]
%     \centering
%     \begin{tabular}{c|cccc}
%         \toprule
%         Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
%         \midrule
%         Commitment & 70.746 & 68.283 & \textcolor{green!50!black}{2.464} & [-0.279, 5.206] \\
%         Bravery & 70.614 & 68.899 & \textcolor{green!50!black}{1.715} & [-1.433, 4.862] \\
%         Mistreatment & 69.645 & 69.276 & \textcolor{green!50!black}{0.369} & [-2.838, 3.575] \\
%         Flags & 68.127 & 69.762 & \textcolor{red!80!black}{-1.634} & [-4.965, 1.697] \\
%         Threat & 68.579 & 69.835 & \textcolor{red!80!black}{-1.255} & [-4.344, 1.833] \\
%         Economy & 69.5 & 69.342 & \textcolor{green!50!black}{0.158} & [-3.013, 3.329]  \\
%         Violation & 69.157 & 69.498 & \textcolor{red!80!black}{-0.341} & [-3.522, 2.839] \\
%         \bottomrule
%     \end{tabular}
%     \caption{Treatment effect estimates using the language modeling approach, where text probabilities under $P^T$ are estimated from pre-trained GPT-2. Text probabilities are computed from marginal sentence probabilities.}
%     \label{tab:results_clm_gpt2_marginal}
% \end{table}

\newpage
\subsubsection{Amazon \cite{przyant2021causal}: Classification approach}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|ccccc}
    \toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$          & $\hat{\tau}_T$          \\
    \midrule
    posemo      &              0.631 &              0.632 & -0.001 [-0.095, 0.093]           & 0.038 [0.001, 0.074]    & 0.067 [0.041, 0.094]    \\
    negemo      &              0.616 &              0.639 & -0.023 [-0.080, 0.034]           & -0.026 [-0.049, -0.003] & -0.040 [-0.059, -0.021] \\
    anx         &              0.629 &              0.631 & -0.003 [-0.120, 0.114]           & -0.052 [-0.095, -0.009] & -0.039 [-0.072, -0.005] \\
    \bottomrule
    \end{tabular}
    \caption{(Amazon \cite{przyant2021causal})  effect estimates using the classification approach, where the classifier is a logistic regression model learned over embeddings from pre-trained MPNet. Here, $P^R$ is the distribution of Amazon reviews for digital music purchases, while $P^T$ is the distribution of Amazon reviews for CD purchases. $\hat{\tau}_{R \rightarrow T}$ is the transported effect estimate from $P^R$ to $P^T$, while $\hat{\tau}_T$ is the effect estimated directly on $P^T$. $g(X), h(X)$ are obtained from LIWC.}
    \label{tab:results_clf_mpnet_amazon_pryzant}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|ccc}
    \toprule
     & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
     \midrule
     Outcome & 0.631 [0.604, 0.659]               & 0.642 [0.632, 0.653] & 0.448 [0.438, 0.457] \\
    \bottomrule
    \end{tabular}
    \caption{(Amazon \cite{przyant2021causal}) Outcome estimates for the effects in Table \ref{tab:results_clf_mpnet_amazon_pryzant}.}
    \label{tab:results_clf_mpnet_amazon_pryzant_outcome}
\end{table}

\subsubsection{Amazon \cite{przyant2021causal} Language modeling approach}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|ccccc}
    \toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$          & $\hat{\tau}_T$          \\
    \midrule
    posemo      &              0.642 &              0.59  & 0.052 [0.000, 0.103]             & 0.038 [0.001, 0.074]    & 0.067 [0.041, 0.094]    \\
    negemo      &              0.627 &              0.641 & -0.013 [-0.050, 0.023]           & -0.026 [-0.049, -0.003] & -0.040 [-0.059, -0.021] \\
    anx         &              0.597 &              0.639 & -0.042 [-0.110, 0.025]           & -0.052 [-0.095, -0.009] & -0.039 [-0.072, -0.005] \\
    \bottomrule
    \end{tabular}
    \caption{(Amazon \cite{przyant2021causal}) Treatment effect estimates using the language modeling approach, where text probabilities under $P^T$ are estimated from GPT-2 fine-tuned with a CLM task on a small number of held-out samples from $P^T$. Here, $P^R$ is the distribution of Amazon reviews for digital music purchases, while $P^T$ is the distribution of Amazon reviews for CD purchases.}
    \label{tab:results_clm_gpt2_finetuned_amazon_pryzant}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
    \toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
    \midrule
    Outcome & 0.637 [0.620, 0.653]               & 0.642 [0.632, 0.653] & 0.448 [0.438, 0.457] \\  
    \bottomrule
    \end{tabular}
    \caption{(Amazon \cite{przyant2021causal}) Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_finetuned_amazon_pryzant}.}
    \label{tab:results_clm_gpt2_finetuned_amazon_pryzant_outcome}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|ccccc}
    \toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$          & $\hat{\tau}_T$          \\
    \midrule
    posemo      &              0.642 &              0.619 & 0.023 [-0.028, 0.075]            & 0.038 [0.001, 0.074]    & 0.067 [0.041, 0.094]    \\
    negemo      &              0.623 &              0.648 & -0.025 [-0.059, 0.009]           & -0.026 [-0.049, -0.003] & -0.040 [-0.059, -0.021] \\
    anx         &              0.589 &              0.644 & -0.055 [-0.124, 0.014]           & -0.052 [-0.095, -0.009] & -0.039 [-0.072, -0.005] \\
    \bottomrule
    \end{tabular}
    \caption{(Amazon \cite{przyant2021causal}) Treatment effect estimates using the language modeling approach, where text probabilities under $P^T$ are estimated from pre-trained GPT-2. Here, $P^R$ is the distribution of Amazon reviews for digital music purchases, while $P^T$ is the distribution of Amazon reviews for CD purchases.}
    \label{tab:results_clm_gpt2_amazon_pryzant}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
    \toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
    \midrule
    Outcome & 0.640 [0.625, 0.656]               & 0.642 [0.632, 0.653] & 0.448 [0.438, 0.457] \\ 
    \bottomrule
    \end{tabular}
    \caption{Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_amazon_pryzant}.}
    \label{tab:results_clm_gpt2_amazon_pryzant_outcome}
\end{table}

\newpage
\subsubsection{Amazon: Classification approach}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccccc}
\toprule
      Treatment &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$       & $\hat{\tau}_T$       \\
\midrule
     sexual      &              7.898 &              2.493 & 5.405 [2.091, 8.718]             & 6.378 [5.308, 7.448]  & 6.703 [5.273, 8.133]   \\
 female      &              8.023 &              2.515 & 5.508 [1.254, 9.762]             & 6.002 [4.967, 7.037]  & 5.410 [4.614, 6.206]   \\
 filler      &              6.375 &              2.556 & 3.819 [-0.071, 7.708]            & 5.847 [4.674, 7.020]  & 7.507 [5.659, 9.356]   \\
 netspeak    &              6.8   &              2.509 & 4.291 [1.591, 6.991]             & 5.416 [4.706, 6.127]  & 5.817 [4.812, 6.823]   \\
 home        &              7.373 &              2.16  & 5.213 [2.236, 8.191]             & 5.167 [4.784, 5.549]  & 4.910 [4.560, 5.260]   \\
 informal    &              3.617 &              2.307 & 1.310 [-0.031, 2.652]            & 2.467 [2.245, 2.689]  & 2.167 [1.937, 2.398]   \\
 shehe       &              4.444 &              2.598 & 1.846 [-0.166, 3.859]            & 0.803 [0.130, 1.475]  & 1.358 [0.777, 1.940]   \\
 nonflu      &              4.115 &              2.438 & 1.677 [0.454, 2.899]             & 2.726 [2.430, 3.022]  & 2.290 [2.001, 2.578]   \\
 assent      &              4.375 &              2.447 & 1.927 [-0.167, 4.021]            & 3.048 [2.653, 3.443]  & 2.681 [2.227, 3.135]   \\
 death       &              1.341 &              2.662 & -1.321 [-3.149, 0.508]           & 0.093 [-0.589, 0.775] & -0.146 [-0.959, 0.667] \\
 \bottomrule
    \end{tabular}
    \caption{(Amazon, direct combination) Treatment effect estimates using the classification approach, where the classifier is a logistic regression learned over embeddings from pre-trained MPNet. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
    \label{tab:results_clf_mpnet_amazon_synthetic}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|ccc}
    \toprule
     & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
 Outcome & 2.626 [1.824, 3.428]               & 4.025 [3.915, 4.136] & 4.101 [4.001, 4.201] \\
\bottomrule
    \end{tabular}
    \caption{(Amazon, direct combination) Outcome estimates for the effects in Table \ref{tab:results_clf_mpnet_amazon_synthetic}.}
    \label{tab:results_clf_mpnet_amazon_synthetic_outcome}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$       & $\hat{\tau}_T$        \\
\midrule
    sexual      &             11.111 &              5.243 & 5.867 [1.200, 10.535]            & 7.404 [5.903, 8.906] & 7.829 [5.792, 9.867]  \\
    female      &             11.863 &              5.254 & 6.610 [1.149, 12.070]            & 7.918 [6.371, 9.465] & 6.550 [5.386, 7.714]  \\
    filler      &             10.091 &              5.3   & 4.791 [-1.555, 11.137]           & 7.209 [5.558, 8.861] & 9.655 [7.052, 12.258] \\
    netspeak    &             10.605 &              5.241 & 5.364 [1.653, 9.076]             & 7.394 [6.322, 8.467] & 7.969 [6.453, 9.484]  \\
    home        &             12.264 &              4.712 & 7.552 [2.126, 12.978]            & 7.179 [6.550, 7.808] & 6.763 [6.180, 7.345]  \\
    informal    &              7.256 &              4.786 & 2.470 [-0.437, 5.378]            & 4.320 [3.941, 4.699] & 4.245 [3.837, 4.652]  \\
    shehe       &              9.865 &              5.318 & 4.547 [0.653, 8.440]             & 3.240 [2.173, 4.307] & 3.031 [2.058, 4.004]  \\
    nonflu      &              7.948 &              5.064 & 2.884 [0.367, 5.401]             & 4.578 [4.081, 5.074] & 4.195 [3.694, 4.696]  \\
    assent      &              8.695 &              5.049 & 3.645 [-0.901, 8.192]            & 4.705 [4.073, 5.337] & 4.624 [3.859, 5.389]  \\
    death       &              5.067 &              5.396 & -0.330 [-3.730, 3.071]           & 2.810 [1.678, 3.942] & 3.303 [1.859, 4.746]  \\
\bottomrule
    \end{tabular}
    \caption{(Amazon, regression combination) Treatment effect estimates using the classification approach, where the classifier is a logistic regression learned over embeddings from pre-trained MPNet. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
    \label{tab:results_clf_mpnet_amazon_synthetic_predreg}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 5.388 [3.584, 7.191]               & 8.067 [7.878, 8.256] & 8.891 [8.710, 9.072] \\
\bottomrule
    \end{tabular}
    \caption{(Amazon, regression combination) Outcome estimates for the effects in Table \ref{tab:results_clf_mpnet_amazon_synthetic_predreg}.}
    \label{tab:results_clf_mpnet_amazon_synthetic_predreg_outcome}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$        & $\hat{\tau}_T$         \\
\midrule
    sexual      &              7.875 &              5.674 & 2.201 [-1.809, 6.211]            & 7.677 [5.050, 10.304] & 10.470 [6.424, 14.516] \\
    female      &             11.744 &              5.603 & 6.140 [1.046, 11.234]            & 8.021 [6.172, 9.870]  & 6.104 [4.825, 7.383]   \\
    filler      &              8.002 &              5.685 & 2.317 [-2.975, 7.609]            & 7.373 [4.922, 9.825]  & 12.269 [8.018, 16.520] \\
    netspeak    &              8.907 &              5.639 & 3.268 [0.299, 6.238]             & 6.759 [5.252, 8.267]  & 9.176 [6.842, 11.510]  \\
    home        &             11.05  &              5.205 & 5.845 [1.427, 10.263]            & 6.564 [5.801, 7.327]  & 6.756 [5.997, 7.514]   \\
    informal    &              7.754 &              5.075 & 2.680 [-0.257, 5.616]            & 4.328 [3.896, 4.759]  & 4.369 [3.891, 4.847]   \\
    shehe       &             10.6   &              5.652 & 4.947 [0.524, 9.371]             & 3.588 [2.304, 4.872]  & 2.464 [1.453, 3.475]   \\
    nonflu      &              7.632 &              5.487 & 2.145 [-0.435, 4.725]            & 4.187 [3.571, 4.803]  & 4.160 [3.540, 4.780]   \\
    assent      &             10.391 &              5.251 & 5.140 [0.735, 9.545]             & 5.075 [4.377, 5.773]  & 4.436 [3.622, 5.251]   \\
    death       &              7.138 &              5.689 & 1.449 [-3.469, 6.366]            & 2.856 [1.624, 4.089]  & 2.399 [0.688, 4.109]   \\
\bottomrule
    \end{tabular}
    \caption{(Amazon, interaction) Treatment effect estimates using the classification approach, where the classifier is a logistic regression learned over embeddings from pre-trained MPNet. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
    \label{tab:results_clf_mpnet_amazon_synthetic_interaction}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 5.728 [3.865, 7.590]               & 8.150 [7.935, 8.364] & 8.817 [8.615, 9.019] \\
\bottomrule
    \end{tabular}
    \caption{(Amazon, interaction) Outcome estimates for the effects in Table \ref{tab:results_clf_mpnet_amazon_synthetic_interaction}.}
    \label{tab:results_clf_mpnet_amazon_synthetic_interaction_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$          & $\hat{\tau}_T$          \\
\midrule
    sexual      &              6.312 &              0.833 & 5.478 [5.090, 5.867]             & 5.712 [4.783, 6.641]    & 5.990 [4.762, 7.219]    \\
    female      &              5.647 &              0.849 & 4.798 [4.371, 5.225]             & 5.070 [4.196, 5.943]    & 4.764 [4.075, 5.454]    \\
    filler      &              5.692 &              0.907 & 4.785 [4.267, 5.302]             & 5.099 [4.088, 6.110]    & 6.420 [4.847, 7.992]    \\
    netspeak    &              4.989 &              0.767 & 4.222 [3.866, 4.578]             & 4.498 [3.901, 5.096]    & 4.821 [3.985, 5.657]    \\
    home        &              4.445 &              0.331 & 4.114 [3.944, 4.283]             & 4.250 [3.935, 4.564]    & 4.052 [3.765, 4.339]    \\
    informal    &              1.914 &              0.446 & 1.468 [1.309, 1.627]             & 1.738 [1.552, 1.923]    & 1.380 [1.189, 1.571]    \\
    shehe       &              0.636 &              1.067 & -0.431 [-0.910, 0.049]           & -0.031 [-0.605, 0.542]  & 0.745 [0.255, 1.234]    \\
    nonflu      &              2.406 &              0.627 & 1.779 [1.583, 1.975]             & 1.987 [1.744, 2.230]    & 1.551 [1.314, 1.787]    \\
    assent      &              2.828 &              0.737 & 2.091 [1.834, 2.348]             & 2.353 [2.023, 2.682]    & 1.908 [1.538, 2.278]    \\
    death       &             -0.087 &              1.098 & -1.185 [-1.674, -0.697]          & -0.795 [-1.375, -0.215] & -1.265 [-1.980, -0.550] \\
\bottomrule
\end{tabular}
\caption{(Lexicon-only sanity check) Treatment effect estimates using the classification approach, where the classifier is a logistic regression learned over binary encodings of LIWC categories. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
\label{tab:results_clf_mpnet_amazon_synthetic_lexonly}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 1.042 [0.968, 1.116]               & 1.367 [1.275, 1.460] & 1.196 [1.113, 1.278] \\
\bottomrule
\end{tabular}
\caption{(Lexicon-only sanity check) Outcome estimates for the effects in Table \ref{tab:results_clf_mpnet_amazon_synthetic_lexonly}.}
\label{tab:results_clf_mpnet_amazon_synthetic_lexonly_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$          & $\hat{\tau}_T$          \\
\midrule
    sexual      &              6.19  &              0.848 & 5.341 [4.911, 5.772]             & 5.712 [4.783, 6.641]    & 5.990 [4.762, 7.219]    \\
    female      &              5.348 &              0.859 & 4.489 [4.026, 4.952]             & 5.070 [4.196, 5.943]    & 4.764 [4.075, 5.454]    \\
    filler      &              5.295 &              0.924 & 4.372 [3.947, 4.796]             & 5.099 [4.088, 6.110]    & 6.420 [4.847, 7.992]    \\
    netspeak    &              4.657 &              0.776 & 3.881 [3.539, 4.224]             & 4.498 [3.901, 5.096]    & 4.821 [3.985, 5.657]    \\
    home        &              4.388 &              0.331 & 4.058 [3.888, 4.227]             & 4.250 [3.935, 4.564]    & 4.052 [3.765, 4.339]    \\
    informal    &              1.843 &              0.447 & 1.396 [1.239, 1.553]             & 1.738 [1.552, 1.923]    & 1.380 [1.189, 1.571]    \\
    shehe       &             -0.245 &              1.08  & -1.326 [-1.724, -0.927]          & -0.031 [-0.605, 0.542]  & 0.745 [0.255, 1.234]    \\
    nonflu      &              2.287 &              0.633 & 1.654 [1.470, 1.838]             & 1.987 [1.744, 2.230]    & 1.551 [1.314, 1.787]    \\
    assent      &              2.651 &              0.745 & 1.905 [1.664, 2.147]             & 2.353 [2.023, 2.682]    & 1.908 [1.538, 2.278]    \\
    death       &             -0.348 &              1.115 & -1.462 [-1.892, -1.032]          & -0.795 [-1.375, -0.215] & -1.265 [-1.980, -0.550] \\
\bottomrule
\end{tabular}
\caption{(Lexicon-only sanity check) Treatment effect estimates using the classification approach, where the classifier is a logistic regression learned over binary encodings of LIWC categories. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products. We use a doubly robust estimator.}
\label{tab:results_clf_mpnet_amazon_synthetic_lexonly_dr}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 1.057 [0.984, 1.131]               & 1.367 [1.275, 1.460] & 1.196 [1.113, 1.278] \\
\bottomrule
\end{tabular}
\caption{(Lexicon-only sanity check) Outcome estimates for the effects in Table \ref{tab:results_clf_mpnet_amazon_synthetic_lexonly_dr}.}
\label{tab:results_clf_mpnet_amazon_synthetic_lexonly_dr_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   & $\hat{\mu}(P_1)$        & $\hat{\mu}(P_0)$        & $\hat{\tau}_{R \rightarrow T}$   \\
\midrule
    sexual      & 5.599 [5.464, 5.738]    & 5.812 [5.672, 5.941]    & 6.024 [5.860, 6.193]             \\
    female      & 4.826 [4.690, 4.966]    & 5.098 [4.963, 5.225]    & 4.713 [4.587, 4.840]             \\
    filler      & 4.884 [4.727, 5.035]    & 5.200 [5.041, 5.344]    & 6.499 [6.303, 6.711]             \\
    netspeak    & 4.177 [4.080, 4.277]    & 4.444 [4.349, 4.546]    & 4.800 [4.668, 4.939]             \\
    home        & 4.066 [4.002, 4.138]    & 4.213 [4.150, 4.281]    & 4.130 [4.059, 4.205]             \\
    informal    & 1.491 [1.435, 1.548]    & 1.759 [1.701, 1.814]    & 1.418 [1.355, 1.476]             \\
    shehe       & -0.381 [-0.496, -0.256] & 0.009 [-0.098, 0.129]   & 0.766 [0.657, 0.886]             \\
    nonflu      & 1.737 [1.670, 1.799]    & 1.939 [1.871, 1.999]    & 1.611 [1.543, 1.680]             \\
    assent      & 2.107 [2.036, 2.180]    & 2.364 [2.292, 2.436]    & 2.004 [1.919, 2.096]             \\
    death       & -1.150 [-1.277, -1.032] & -0.746 [-0.868, -0.624] & -1.109 [-1.280, -0.939]          \\
\bottomrule
\end{tabular}
\caption{(Lexicon-only sanity check, bootstrap) Treatment effect estimates using the classification approach, where the classifier is a logistic regression learned over binary encodings of LIWC categories. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products. We use a doubly robust estimator and obtain the confidence intervals by bootstrap resampling noise for the synthetic label.}
\label{tab:results_clf_mpnet_amazon_synthetic_lexonly_dr_bootstrap}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 1.047 [1.018, 1.077]               & 1.372 [1.344, 1.402] & 1.204 [1.177, 1.232] \\
\bottomrule
\end{tabular}
\caption{(Lexicon-only sanity check, bootstrap) Outcome estimates for the effects in Table \ref{tab:results_clf_mpnet_amazon_synthetic_lexonly_dr_bootstrap}.}
\label{tab:results_clf_mpnet_amazon_synthetic_lexonly_dr_bootstrap_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   & $\hat{\mu}(P_1)$        & $\hat{\mu}(P_0)$        & $\hat{\tau}_{R \rightarrow T}$   \\
\midrule
    sexual      & 6.040 [5.861, 6.206]    & 6.178 [6.004, 6.334]    & 6.130 [5.924, 6.321]             \\
    female      & 4.515 [4.378, 4.668]    & 4.824 [4.684, 4.981]    & 4.713 [4.574, 4.842]             \\
    filler      & 5.324 [5.159, 5.483]    & 5.507 [5.341, 5.653]    & 5.492 [5.272, 5.722]             \\
    netspeak    & 4.694 [4.590, 4.810]    & 4.787 [4.682, 4.901]    & 4.629 [4.490, 4.767]             \\
    home        & 3.987 [3.905, 4.071]    & 4.216 [4.135, 4.298]    & 4.051 [3.974, 4.135]             \\
    informal    & 1.449 [1.383, 1.518]    & 1.778 [1.710, 1.846]    & 1.392 [1.329, 1.456]             \\
    shehe       & 0.668 [0.528, 0.807]    & 0.560 [0.433, 0.695]    & 0.311 [0.172, 0.433]             \\
    nonflu      & 1.751 [1.667, 1.827]    & 2.110 [2.030, 2.181]    & 1.614 [1.543, 1.690]             \\
    assent      & 2.078 [1.994, 2.158]    & 2.192 [2.106, 2.271]    & 2.104 [2.022, 2.204]             \\
    death       & -1.234 [-1.391, -1.079] & -0.890 [-1.040, -0.742] & -1.153 [-1.307, -0.981]          \\
\bottomrule
\end{tabular}
\caption{(``Contrived'' setting, bootstrap) Treatment effect estimates using the classification approach, where the classifier is a logistic regression learned over binary encodings of LIWC categories. $P^R$ and $P^T$ are as defined in Section \ref{sec:contrived_data}. We use a doubly robust estimator (we note that this produces exactly the same results as a difference-in-means estimator) and obtain the confidence intervals by bootstrap resampling noise for the synthetic label.}
\label{tab:results_clf_liwc_contrived_bootstrap}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 1.163 [1.130, 1.196]               & 1.365 [1.334, 1.399] & 1.189 [1.160, 1.221] \\
\bottomrule
\end{tabular}
\caption{(``Contrived'' setting, bootstrap) Outcome estimates for the effects in Table \ref{tab:results_clf_liwc_contrived_bootstrap}.}
\label{tab:results_clf_liwc_contrived_bootstrap_outcome}
\end{table}

\newpage
\subsubsection{Amazon: Language modeling approach}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccccc}
\toprule
      Treatment &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$       & $\hat{\tau}_T$       \\
\midrule
 sexual      &             10.327 &              3.975 & 6.352 [5.226, 7.478]             & 6.378 [5.308, 7.448]  & 6.703 [5.273, 8.133]   \\
 female      &              9.955 &              3.997 & 5.959 [4.811, 7.106]             & 6.002 [4.967, 7.037]  & 5.410 [4.614, 6.206]   \\
 filler      &              9.637 &              4.072 & 5.565 [4.391, 6.738]             & 5.847 [4.674, 7.020]  & 7.507 [5.659, 9.356]   \\
 netspeak    &              9.142 &              3.796 & 5.346 [4.543, 6.150]             & 5.416 [4.706, 6.127]  & 5.817 [4.812, 6.823]   \\
 home        &              8.277 &              3.163 & 5.113 [4.687, 5.539]             & 5.167 [4.784, 5.549]  & 4.910 [4.560, 5.260]   \\
 informal    &              5.567 &              2.91  & 2.657 [2.363, 2.951]             & 2.467 [2.245, 2.689]  & 2.167 [1.937, 2.398]   \\
 shehe       &              4.998 &              4.242 & 0.756 [-0.044, 1.556]            & 0.803 [0.130, 1.475]  & 1.358 [0.777, 1.940]   \\
 nonflu      &              6.173 &              3.437 & 2.736 [2.350, 3.121]             & 2.726 [2.430, 3.022]  & 2.290 [2.001, 2.578]   \\
 assent      &              6.692 &              3.691 & 3.001 [2.511, 3.491]             & 3.048 [2.653, 3.443]  & 2.681 [2.227, 3.135]   \\
 death       &              4.107 &              4.303 & -0.196 [-0.962, 0.570]           & 0.093 [-0.589, 0.775] & -0.146 [-0.959, 0.667] \\
\bottomrule
    \end{tabular}
    \caption{(Amazon, direct combination) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from pre-trained GPT-2. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
    \label{tab:results_clm_gpt2_amazon_synthetic}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|ccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
 Outcome & 4.291 [4.134, 4.448]               & 4.025 [3.915, 4.136] & 4.101 [4.001, 4.201] \\
\bottomrule
\end{tabular}
\caption{(Amazon, direct combination) Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_amazon_synthetic}.}
\label{tab:results_clm_gpt2_amazon_synthetic_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$       & $\hat{\tau}_T$        \\
\midrule
    sexual      &             14.769 &              7.919 & 6.850 [5.262, 8.438]             & 7.404 [5.903, 8.906] & 7.829 [5.792, 9.867]  \\
    female      &             15.389 &              7.899 & 7.490 [5.907, 9.073]             & 7.918 [6.371, 9.465] & 6.550 [5.386, 7.714]  \\
    filler      &             14.476 &              8.007 & 6.470 [4.847, 8.092]             & 7.209 [5.558, 8.861] & 9.655 [7.052, 12.258] \\
    netspeak    &             14.659 &              7.64  & 7.019 [5.866, 8.171]             & 7.394 [6.322, 8.467] & 7.969 [6.453, 9.484]  \\
    home        &             13.796 &              6.797 & 6.999 [6.296, 7.702]             & 7.179 [6.550, 7.808] & 6.763 [6.180, 7.345]  \\
    informal    &             10.279 &              6.164 & 4.116 [3.645, 4.586]             & 4.320 [3.941, 4.699] & 4.245 [3.837, 4.652]  \\
    shehe       &             10.612 &              8.08  & 2.532 [1.360, 3.703]             & 3.240 [2.173, 4.307] & 3.031 [2.058, 4.004]  \\
    nonflu      &             11.156 &              6.98  & 4.176 [3.586, 4.765]             & 4.578 [4.081, 5.074] & 4.195 [3.694, 4.696]  \\
    assent      &             11.807 &              7.42  & 4.386 [3.632, 5.141]             & 4.705 [4.073, 5.337] & 4.624 [3.859, 5.389]  \\
    death       &             10.748 &              8.092 & 2.656 [1.266, 4.046]             & 2.810 [1.678, 3.942] & 3.303 [1.859, 4.746]  \\
\bottomrule
\end{tabular}
\caption{(Amazon, regression combination) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from GPT-2 fine-tuned with CLM on $P^R$. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
\label{tab:results_clm_gpt2_amazon_synthetic_predreg_finetuned_pr}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 8.241 [7.996, 8.485]               & 8.067 [7.878, 8.256] & 8.891 [8.710, 9.072] \\
\bottomrule
\end{tabular}
\caption{(Amazon, regression combination) Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_amazon_synthetic_predreg_finetuned_pr}.}
\label{tab:results_clm_gpt2_amazon_synthetic_predreg_finetuned_pr_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$       & $\hat{\tau}_T$        \\
\midrule
    sexual      &             14.938 &              7.965 & 6.973 [5.398, 8.548]             & 7.404 [5.903, 8.906] & 7.829 [5.792, 9.867]  \\
    female      &             15.441 &              7.951 & 7.490 [5.897, 9.083]             & 7.918 [6.371, 9.465] & 6.550 [5.386, 7.714]  \\
    filler      &             14.659 &              8.054 & 6.605 [4.929, 8.280]             & 7.209 [5.558, 8.861] & 9.655 [7.052, 12.258] \\
    netspeak    &             14.677 &              7.692 & 6.985 [5.877, 8.093]             & 7.394 [6.322, 8.467] & 7.969 [6.453, 9.484]  \\
    home        &             13.89  &              6.831 & 7.059 [6.337, 7.781]             & 7.179 [6.550, 7.808] & 6.763 [6.180, 7.345]  \\
    informal    &             10.354 &              6.182 & 4.172 [3.696, 4.648]             & 4.320 [3.941, 4.699] & 4.245 [3.837, 4.652]  \\
    shehe       &             10.729 &              8.129 & 2.600 [1.406, 3.793]             & 3.240 [2.173, 4.307] & 3.031 [2.058, 4.004]  \\
    nonflu      &             11.234 &              7.014 & 4.221 [3.628, 4.814]             & 4.578 [4.081, 5.074] & 4.195 [3.694, 4.696]  \\
    assent      &             11.901 &              7.459 & 4.442 [3.685, 5.199]             & 4.705 [4.073, 5.337] & 4.624 [3.859, 5.389]  \\
    death       &             10.777 &              8.148 & 2.629 [1.287, 3.972]             & 2.810 [1.678, 3.942] & 3.303 [1.859, 4.746]  \\
\bottomrule
\end{tabular}
\caption{(Amazon, regression combination) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from GPT-2 fine-tuned with CLM on $P^T$. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
\label{tab:results_clm_gpt2_amazon_synthetic_predreg_finetuned_pt}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 8.296 [8.048, 8.544]               & 8.067 [7.878, 8.256] & 8.891 [8.710, 9.072] \\
\bottomrule
\end{tabular}
\caption{(Amazon, regression combination) Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_amazon_synthetic_predreg_finetuned_pt}.}
\label{tab:results_clm_gpt2_amazon_synthetic_predreg_finetuned_pt_outcome}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$       & $\hat{\tau}_T$        \\
\midrule
    sexual      &             15.696 &              8.363 & 7.333 [5.410, 9.255]             & 7.404 [5.903, 8.906] & 7.829 [5.792, 9.867]  \\
    female      &             16.423 &              8.328 & 8.095 [6.040, 10.150]            & 7.918 [6.371, 9.465] & 6.550 [5.386, 7.714]  \\
    filler      &             15.257 &              8.46  & 6.797 [4.868, 8.725]             & 7.209 [5.558, 8.861] & 9.655 [7.052, 12.258] \\
    netspeak    &             15.317 &              8.055 & 7.262 [5.889, 8.635]             & 7.394 [6.322, 8.467] & 7.969 [6.453, 9.484]  \\
    home        &             14.392 &              7.125 & 7.267 [6.456, 8.078]             & 7.179 [6.550, 7.808] & 6.763 [6.180, 7.345]  \\
    informal    &             10.931 &              6.343 & 4.588 [4.036, 5.140]             & 4.320 [3.941, 4.699] & 4.245 [3.837, 4.652]  \\
    shehe       &             11.675 &              8.523 & 3.152 [1.736, 4.568]             & 3.240 [2.173, 4.307] & 3.031 [2.058, 4.004]  \\
    nonflu      &             11.919 &              7.279 & 4.640 [3.931, 5.349]             & 4.578 [4.081, 5.074] & 4.195 [3.694, 4.696]  \\
    assent      &             12.393 &              7.811 & 4.582 [3.719, 5.444]             & 4.705 [4.073, 5.337] & 4.624 [3.859, 5.389]  \\
    death       &             11.231 &              8.563 & 2.668 [1.158, 4.178]             & 2.810 [1.678, 3.942] & 3.303 [1.859, 4.746]  \\
 \bottomrule
    \end{tabular}
    \caption{(Amazon, regression combination) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from pre-trained GPT-2. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
    \label{tab:results_clm_gpt2_amazon_synthetic_predreg}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
\toprule
     & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 8.727 [8.435, 9.019]               & 8.067 [7.878, 8.256] & 8.891 [8.710, 9.072] \\
\bottomrule
    \end{tabular}
    \caption{Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_amazon_synthetic_predreg}.}
    \label{tab:results_clm_gpt2_amazon_synthetic_predreg_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$        & $\hat{\tau}_T$         \\
\midrule
    sexual      &             15.237 &              7.988 & 7.249 [4.378, 10.120]            & 7.677 [5.050, 10.304] & 10.470 [6.424, 14.516] \\
    female      &             15.763 &              7.974 & 7.790 [5.748, 9.831]             & 8.021 [6.172, 9.870]  & 6.104 [4.825, 7.383]   \\
    filler      &             14.804 &              8.086 & 6.718 [4.137, 9.298]             & 7.373 [4.922, 9.825]  & 12.269 [8.018, 16.520] \\
    netspeak    &             14.079 &              7.787 & 6.291 [4.692, 7.891]             & 6.759 [5.252, 8.267]  & 9.176 [6.842, 11.510]  \\
    home        &             13.42  &              7.004 & 6.416 [5.564, 7.268]             & 6.564 [5.801, 7.327]  & 6.756 [5.997, 7.514]   \\
    informal    &             10.413 &              6.197 & 4.216 [3.690, 4.742]             & 4.328 [3.896, 4.759]  & 4.369 [3.891, 4.847]   \\
    shehe       &             10.784 &              8.166 & 2.618 [1.360, 3.876]             & 3.588 [2.304, 4.872]  & 2.464 [1.453, 3.475]   \\
    nonflu      &             11.023 &              7.164 & 3.859 [3.144, 4.574]             & 4.187 [3.571, 4.803]  & 4.160 [3.540, 4.780]   \\
    assent      &             12.053 &              7.47  & 4.584 [3.825, 5.342]             & 5.075 [4.377, 5.773]  & 4.436 [3.622, 5.251]   \\
    death       &             10.361 &              8.213 & 2.147 [0.822, 3.473]             & 2.856 [1.624, 4.089]  & 2.399 [0.688, 4.109]   \\
\bottomrule
\end{tabular}
\caption{(Amazon, interaction) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from GPT-2 fine-tuned using CLM on $P^R$. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
\label{tab:results_clm_gpt2_amazon_synthetic_interaction_finetuned_pr}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 8.295 [8.025, 8.565]               & 8.150 [7.935, 8.364] & 8.817 [8.615, 9.019] \\
\bottomrule
\end{tabular}
\caption{Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_amazon_synthetic_interaction_finetuned_pr}.}
\label{tab:results_clm_gpt2_amazon_synthetic_interaction_finetuned_pr_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$        & $\hat{\tau}_T$         \\
\midrule
    sexual      &             15.237 &              7.988 & 7.249 [4.378, 10.120]            & 7.677 [5.050, 10.304] & 10.470 [6.424, 14.516] \\
    female      &             15.763 &              7.974 & 7.790 [5.748, 9.831]             & 8.021 [6.172, 9.870]  & 6.104 [4.825, 7.383]   \\
    filler      &             14.804 &              8.086 & 6.718 [4.137, 9.298]             & 7.373 [4.922, 9.825]  & 12.269 [8.018, 16.520] \\
    netspeak    &             14.079 &              7.787 & 6.291 [4.692, 7.891]             & 6.759 [5.252, 8.267]  & 9.176 [6.842, 11.510]  \\
    home        &             13.42  &              7.004 & 6.416 [5.564, 7.268]             & 6.564 [5.801, 7.327]  & 6.756 [5.997, 7.514]   \\
    informal    &             10.413 &              6.197 & 4.216 [3.690, 4.742]             & 4.328 [3.896, 4.759]  & 4.369 [3.891, 4.847]   \\
    shehe       &             10.784 &              8.166 & 2.618 [1.360, 3.876]             & 3.588 [2.304, 4.872]  & 2.464 [1.453, 3.475]   \\
    nonflu      &             11.023 &              7.164 & 3.859 [3.144, 4.574]             & 4.187 [3.571, 4.803]  & 4.160 [3.540, 4.780]   \\
    assent      &             12.053 &              7.47  & 4.584 [3.825, 5.342]             & 5.075 [4.377, 5.773]  & 4.436 [3.622, 5.251]   \\
    death       &             10.361 &              8.213 & 2.147 [0.822, 3.473]             & 2.856 [1.624, 4.089]  & 2.399 [0.688, 4.109]   \\
\bottomrule
\end{tabular}
\caption{(Amazon, interaction) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from GPT-2 fine-tuned using CLM on $P^T$. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
\label{tab:results_clm_gpt2_amazon_synthetic_interaction_finetuned_pt}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 8.334 [8.061, 8.608]               & 8.150 [7.935, 8.364] & 8.817 [8.615, 9.019] \\
\bottomrule
\end{tabular}
\caption{Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_amazon_synthetic_interaction_finetuned_pt}.}
\label{tab:results_clm_gpt2_amazon_synthetic_interaction_finetuned_pt_outcome}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$        & $\hat{\tau}_T$         \\
\midrule
    sexual      &             16.77  &              8.346 & 8.423 [4.982, 11.865]            & 7.677 [5.050, 10.304] & 10.470 [6.424, 14.516] \\
    female      &             16.742 &              8.35  & 8.392 [5.902, 10.882]            & 8.021 [6.172, 9.870]  & 6.104 [4.825, 7.383]   \\
    filler      &             15.712 &              8.48  & 7.232 [4.311, 10.152]            & 7.373 [4.922, 9.825]  & 12.269 [8.018, 16.520] \\
    netspeak    &             15.045 &              8.123 & 6.921 [4.953, 8.889]             & 6.759 [5.252, 8.267]  & 9.176 [6.842, 11.510]  \\
    home        &             13.993 &              7.285 & 6.708 [5.770, 7.647]             & 6.564 [5.801, 7.327]  & 6.756 [5.997, 7.514]   \\
    informal    &             11.003 &              6.342 & 4.661 [4.048, 5.275]             & 4.328 [3.896, 4.759]  & 4.369 [3.891, 4.847]   \\
    shehe       &             11.59  &              8.568 & 3.021 [1.573, 4.470]             & 3.588 [2.304, 4.872]  & 2.464 [1.453, 3.475]   \\
    nonflu      &             11.749 &              7.41  & 4.339 [3.492, 5.186]             & 4.187 [3.571, 4.803]  & 4.160 [3.540, 4.780]   \\
    assent      &             12.47  &              7.838 & 4.633 [3.729, 5.537]             & 5.075 [4.377, 5.773]  & 4.436 [3.622, 5.251]   \\
    death       &             10.859 &              8.627 & 2.232 [0.504, 3.959]             & 2.856 [1.624, 4.089]  & 2.399 [0.688, 4.109]   \\
\bottomrule
    \end{tabular}
    \caption{(Amazon, interaction) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from pre-trained GPT-2. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
    \label{tab:results_clm_gpt2_amazon_synthetic_interaction}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 8.764 [8.440, 9.089]               & 8.150 [7.935, 8.364] & 8.817 [8.615, 9.019] \\
\bottomrule
    \end{tabular}
    \caption{Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_amazon_synthetic_interaction}.}
    \label{tab:results_clm_gpt2_amazon_synthetic_interaction_outcome}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$        & $\hat{\tau}_T$         \\
\midrule
    sexual      &             15.776 &              7.816 & 7.960 [5.373, 10.548]            & 7.677 [5.050, 10.304] & 10.470 [6.424, 14.516] \\
    female      &             15.818 &              7.815 & 8.003 [6.448, 9.559]             & 8.021 [6.172, 9.870]  & 6.104 [4.825, 7.383]   \\
    filler      &             15.188 &              7.904 & 7.284 [5.036, 9.532]             & 7.373 [4.922, 9.825]  & 12.269 [8.018, 16.520] \\
    netspeak    &             14.175 &              7.598 & 6.578 [5.206, 7.949]             & 6.759 [5.252, 8.267]  & 9.176 [6.842, 11.510]  \\
    home        &             13.45  &              6.843 & 6.608 [5.882, 7.334]             & 6.564 [5.801, 7.327]  & 6.756 [5.997, 7.514]   \\
    informal    &             10.388 &              6.002 & 4.386 [3.953, 4.818]             & 4.328 [3.896, 4.759]  & 4.369 [3.891, 4.847]   \\
    shehe       &             11.523 &              7.951 & 3.572 [2.291, 4.853]             & 3.588 [2.304, 4.872]  & 2.464 [1.453, 3.475]   \\
    nonflu      &             11.183 &              6.929 & 4.254 [3.632, 4.876]             & 4.187 [3.571, 4.803]  & 4.160 [3.540, 4.780]   \\
    assent      &             12.255 &              7.23  & 5.025 [4.363, 5.688]             & 5.075 [4.377, 5.773]  & 4.436 [3.622, 5.251]   \\
    death       &             10.759 &              8.033 & 2.726 [1.549, 3.903]             & 2.856 [1.624, 4.089]  & 2.399 [0.688, 4.109]   \\
\bottomrule
    \end{tabular}
    \caption{(Amazon, interaction) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from GPT-2 fine-tuned on $P^T$ and text probabilities under $P^R$ are estimated from GPT-2 fine-tuned on $P^R$. Here, $P^R$ is the distribution of Amazon reviews for musical instrument purchases, while $P^T$ is the distribution of Amazon reviews for office products.}
    \label{tab:results_clm_gpt2_finetuned_amazon_synthetic_interaction_ptpr}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 8.185 [7.959, 8.410]               & 8.150 [7.935, 8.364] & 8.817 [8.615, 9.019] \\
\bottomrule
    \end{tabular}
    \caption{Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_finetuned_amazon_synthetic_interaction_ptpr}.}
    \label{tab:results_clm_gpt2_finetuned_amazon_synthetic_interaction_outcome_ptpr}
\end{table}


\newpage
\subsubsection{EmoBank: Classification approach}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$          & $\hat{\tau}_T$          \\
\midrule
    posemo      &              3.106 &              2.74  & 0.366 [0.226, 0.506]             & 0.352 [0.314, 0.390]    & 0.323 [0.280, 0.367]    \\
    negemo      &              2.599 &              2.963 & -0.364 [-0.509, -0.220]          & -0.399 [-0.458, -0.340] & -0.375 [-0.418, -0.333] \\
    anx         &              2.566 &              2.876 & -0.310 [-0.620, -0.000]          & -0.439 [-0.580, -0.298] & -0.276 [-0.359, -0.192] \\
    nonflu      &              3.112 &              2.853 & 0.258 [-0.184, 0.700]            & 0.159 [0.046, 0.273]    & 0.178 [0.043, 0.313]    \\
    filler      &              3.064 &              2.857 & 0.207 [0.019, 0.395]             & -0.019 [-0.129, 0.092]  & 0.199 [-0.191, 0.588]   \\
    home        &              2.827 &              2.86  & -0.033 [-0.307, 0.241]           & 0.056 [-0.018, 0.130]   & 0.048 [-0.027, 0.124]   \\
    assent      &              3.128 &              2.855 & 0.273 [-0.182, 0.728]            & 0.239 [0.057, 0.421]    & 0.332 [0.110, 0.554]    \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Treatment effects using the classification approach, where the classifier is a logistic regression learned over pre-trained MPNet embeddings. $P^R$ is the distribution of sentences with high writer-intended valence, and $P^T$ is the distribution of sentences with low writer-intended valence. The measured outcome $Y$ is reader-perceived valence.}
\label{tab:results_clf_mpnet_emobank}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 2.858 [2.791, 2.924]               & 3.085 [3.067, 3.104] & 2.801 [2.782, 2.820] \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Outcome estimates for the effects in Table \ref{tab:results_clf_mpnet_emobank}.}
\label{tab:results_clf_mpnet_emobank_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$          & $\hat{\tau}_T$          \\
\midrule
    posemo & 3.078 &              2.746             & 0.332 [0.291, 0.369]             & 0.343 [0.317, 0.366]    & 0.317 [0.290, 0.344]    \\
    negemo & 2.58  &              2.966             & -0.386 [-0.424, -0.347]          & -0.422 [-0.456, -0.389] & -0.354 [-0.378, -0.330] \\
    anx & 2.576 &              2.872                & -0.297 [-0.385, -0.209]          & -0.392 [-0.464, -0.322] & -0.269 [-0.313, -0.230] \\
    nonflu & 3.044 &              2.852             & 0.193 [0.086, 0.304]             & 0.133 [0.059, 0.211]    & 0.170 [0.076, 0.250]    \\
    3.069 &              2.855 & filler             & 0.214 [0.132, 0.353]             & -0.011 [-0.095, 0.119]  & 0.228 [0.193, 0.311]    \\
    home & 2.851 &              2.855              & -0.004 [-0.088, 0.088]           & 0.044 [0.001, 0.090]    & 0.021 [-0.027, 0.075]   \\
    assent & 3.114 &              2.852             & 0.262 [0.105, 0.427]             & 0.221 [0.117, 0.327]    & 0.201 [0.058, 0.340]    \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Treatment effects using the classification approach, where the classifier is a logistic regression learned over pre-trained MPNet embeddings. $P^R$ is the distribution of sentences with high writer-intended valence, and $P^T$ is the distribution of sentences with low writer-intended valence. The measured outcome $Y$ is reader-perceived valence. We obtain confidence intervals by bootstrap resampling $P^R$ and $P^T$ probabilistically, according to the heuristic used to sample the original $P^R$ and $P^T$.}
\label{tab:results_clf_mpnet_emobank_boot}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 2.855 [2.835, 2.873]               & 3.085 [3.074, 3.098] & 2.796 [2.784, 2.809] \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Outcome estimates for the effects in Table \ref{tab:results_clf_mpnet_emobank}.}
\label{tab:results_clf_mpnet_emobank_boot_outcome}
\end{table}

\newpage
\subsubsection{EmoBank: Language modeling approach}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$          & $\hat{\tau}_T$          \\
\midrule
    posemo      &              3.283 &              2.927 & 0.356 [0.190, 0.523]             & 0.352 [0.314, 0.390]    & 0.323 [0.280, 0.367]    \\
    negemo      &              2.739 &              3.143 & -0.404 [-0.593, -0.216]          & -0.399 [-0.458, -0.340] & -0.375 [-0.418, -0.333] \\
    anx         &              2.632 &              3.086 & -0.453 [-0.868, -0.038]          & -0.439 [-0.580, -0.298] & -0.276 [-0.359, -0.192] \\
    nonflu      &              3.212 &              3.068 & 0.144 [-0.472, 0.760]            & 0.159 [0.046, 0.273]    & 0.178 [0.043, 0.313]    \\
    filler      &              3.109 &              3.071 & 0.038 [-1.681, 1.758]            & -0.019 [-0.129, 0.092]  & 0.199 [-0.191, 0.588]   \\
    home        &              3.106 &              3.069 & 0.038 [-0.249, 0.324]            & 0.056 [-0.018, 0.130]   & 0.048 [-0.027, 0.124]   \\
    assent      &              3.331 &              3.067 & 0.264 [-0.433, 0.960]            & 0.239 [0.057, 0.421]    & 0.332 [0.110, 0.554]    \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from GPT-2 fine-tuned using CLM on $P^R$. $P^R$ is the distribution of sentences with high writer-intended valence, and $P^T$ is the distribution of sentences with low writer-intended valence. The measured outcome $Y$ is reader-perceived valence.}
\label{tab:results_clm_gpt2_finetuned_emobank_pr}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 3.071 [2.991, 3.151]               & 3.085 [3.067, 3.104] & 2.801 [2.782, 2.820] \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_finetuned_emobank_pr}.}
\label{tab:results_clm_gpt2_finetuned_emobank_pr_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$          & $\hat{\tau}_T$          \\
\midrule
    posemo      &              3.274 &              2.931 & 0.343 [0.180, 0.507]             & 0.352 [0.314, 0.390]    & 0.323 [0.280, 0.367]    \\
    negemo      &              2.745 &              3.145 & -0.400 [-0.585, -0.215]          & -0.399 [-0.458, -0.340] & -0.375 [-0.418, -0.333] \\
    anx         &              2.665 &              3.087 & -0.422 [-0.836, -0.007]          & -0.439 [-0.580, -0.298] & -0.276 [-0.359, -0.192] \\
    nonflu      &              3.207 &              3.07  & 0.137 [-0.433, 0.707]            & 0.159 [0.046, 0.273]    & 0.178 [0.043, 0.313]    \\
    filler      &              3.097 &              3.073 & 0.024 [-1.306, 1.353]            & -0.019 [-0.129, 0.092]  & 0.199 [-0.191, 0.588]   \\
    home        &              3.106 &              3.071 & 0.035 [-0.259, 0.329]            & 0.056 [-0.018, 0.130]   & 0.048 [-0.027, 0.124]   \\
    assent      &              3.325 &              3.069 & 0.256 [-0.453, 0.965]            & 0.239 [0.057, 0.421]    & 0.332 [0.110, 0.554]    \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from GPT-2 fine-tuned using CLM on $P^T$. $P^R$ is the distribution of sentences with high writer-intended valence, and $P^T$ is the distribution of sentences with low writer-intended valence. The measured outcome $Y$ is reader-perceived valence.}
\label{tab:results_clm_gpt2_finetuned_emobank_pt}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 3.073 [2.994, 3.151]               & 3.085 [3.067, 3.104] & 2.801 [2.782, 2.820] \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_finetuned_emobank_pt}.}
\label{tab:results_clm_gpt2_finetuned_emobank_pt_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$          & $\hat{\tau}_T$          \\
\midrule
    posemo      &              3.295 &              2.941 & 0.354 [0.229, 0.480]             & 0.352 [0.314, 0.390]    & 0.323 [0.280, 0.367]    \\
    negemo      &              2.784 &              3.163 & -0.379 [-0.531, -0.226]          & -0.399 [-0.458, -0.340] & -0.375 [-0.418, -0.333] \\
    anx         &              2.678 &              3.107 & -0.429 [-0.762, -0.096]          & -0.439 [-0.580, -0.298] & -0.276 [-0.359, -0.192] \\
    nonflu      &              3.218 &              3.089 & 0.129 [-0.320, 0.577]            & 0.159 [0.046, 0.273]    & 0.178 [0.043, 0.313]    \\
    filler      &              3.11  &              3.091 & 0.018 [-1.882, 1.919]            & -0.019 [-0.129, 0.092]  & 0.199 [-0.191, 0.588]   \\
    home        &              3.166 &              3.086 & 0.080 [-0.154, 0.313]            & 0.056 [-0.018, 0.130]   & 0.048 [-0.027, 0.124]   \\
    assent      &              3.291 &              3.089 & 0.201 [-0.450, 0.853]            & 0.239 [0.057, 0.421]    & 0.332 [0.110, 0.554]    \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from pre-trained GPT-2. $P^R$ is the distribution of sentences with high writer-intended valence, and $P^T$ is the distribution of sentences with low writer-intended valence. The measured outcome $Y$ is reader-perceived valence.}
\label{tab:results_clm_gpt2_pretrained_emobank}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 3.092 [3.029, 3.154]               & 3.085 [3.067, 3.104] & 2.801 [2.782, 2.820] \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_pretrained_emobank}.}
\label{tab:results_clm_gpt2_pretrained_emobank_outcome}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccccc}
\toprule
    Treatment   &   $\hat{\mu}(P_1)$ &   $\hat{\mu}(P_0)$ & $\hat{\tau}_{R \rightarrow T}$   & $\hat{\tau}_R$          & $\hat{\tau}_T$          \\
\midrule
    posemo      &              3.279 &              2.94  & 0.339 [0.246, 0.433]             & 0.352 [0.314, 0.390]    & 0.323 [0.280, 0.367]    \\
    negemo      &              2.767 &              3.152 & -0.384 [-0.505, -0.264]          & -0.399 [-0.458, -0.340] & -0.375 [-0.418, -0.333] \\
    anx         &              2.692 &              3.096 & -0.404 [-0.673, -0.136]          & -0.439 [-0.580, -0.298] & -0.276 [-0.359, -0.192] \\
    nonflu      &              3.25  &              3.079 & 0.171 [-0.237, 0.579]            & 0.159 [0.046, 0.273]    & 0.178 [0.043, 0.313]    \\
    filler      &              3.056 &              3.083 & -0.027 [-0.623, 0.570]           & -0.019 [-0.129, 0.092]  & 0.199 [-0.191, 0.588]   \\
    home        &              3.122 &              3.08  & 0.043 [-0.146, 0.232]            & 0.056 [-0.018, 0.130]   & 0.048 [-0.027, 0.124]   \\
    assent      &              3.301 &              3.08  & 0.221 [-0.121, 0.564]            & 0.239 [0.057, 0.421]    & 0.332 [0.110, 0.554]    \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Treatment effects using the language modeling approach, where text probabilities under $P^T$ are estimated from GPT-2 fine-tuned on $P^T$ and text probabilities under $P^R$ are estimated from GPT-2 fine-tuned on $P^R$. $P^R$ is the distribution of sentences with high writer-intended valence, and $P^T$ is the distribution of sentences with low writer-intended valence. The measured outcome $Y$ is reader-perceived valence.}
\label{tab:results_clm_gpt2_finetuned_emobank_ptpr}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{c|cccc}
\toprule
    & $\hat{\mu}(P_{R \rightarrow T})$   & $\hat{\mu}(P_R)$     & $\hat{\mu}(P_T)$     \\
\midrule
    Outcome & 3.083 [3.036, 3.129]               & 3.085 [3.067, 3.104] & 2.801 [2.782, 2.820] \\
\bottomrule
\end{tabular}
\caption{(EmoBank) Outcome estimates for the effects in Table \ref{tab:results_clm_gpt2_finetuned_emobank_ptpr}.}
\label{tab:results_clm_gpt2_finetuned_emobank_outcome_ptpr}
\end{table}

\newpage
\subsection{Discussion}

\vl{Marginal probabilities seem to produce less stable results, so we will disregard those for now.}

We obtain similar results across both the classification and language modeling approaches.  As in the original study, we find \textit{commitment} to have a significant positive effect on the response and other text attributes to have no significant effect (although the direction of the effect is generally consistent with the original study). Unlike the original study, we find \textit{bravery} to additionally have a significant positive effect on the response across both approaches.

\subsection{Aliasing check}

For a randomized study to isolate the effect of one text attribute, the texts must be parameterized such that there is no \textit{aliasing} across the attributes. That is, the following assumption must hold: either all the attributes must be independent, or the unmeasured attributes must be unrelated to the outcome. If the treatment of interest is aliased, it may vary simultaneously with other text attributes, both measured and unmeasured.

Fong \& Grimmer claim that (1) the construction of their texts prevents aliasing of measured attributes, but they acknowledge that (2) aliasing of the treatment with unmeasured text attributes, such as the phrasing of sentences or other attributes specific to the randomization text distribution, is still possible. Our work is meant in part to address (2). However, we should check that (1) holds as well. 

One particular concern is that even if (1) holds for the natural language representation of the text, it may not hold for the language representations that we are using experimentally (e.g., MPNet or GPT-2 embeddings). We can check the validity of the claim using t-SNE plots of the candidate sentence embeddings for each text attribute. If the sentence embeddings clearly cluster by text attribute, we can be more confident that the attributes are independent. However, this is not a perfect measure, since for each individual attribute, the wording of the candidate sentences is similar, making it fundamentally more likely for sentences to cluster by attribute.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{plots/sanity_check/all-mpnet-base-v2.png}
    \includegraphics[width=0.45\textwidth]{plots/sanity_check/bert-base-uncased.png}
    \includegraphics[width=0.45\textwidth]{plots/sanity_check/gpt2.png}
    \caption{t-SNE plots of sentence embeddings color-coded by the associated text attribute.}
    \label{fig:alias_check}
\end{figure}

We construct t-SNE plots of MPNet, BERT \cite{devlin2019bert}, and GPT-2 embeddings (Figure \ref{fig:alias_check}). Currently, we use MPNet embeddings in our classification approach and GPT-2 embeddings in our language modeling approach. While we see that MPNet and BERT embeddings separate clearly by text attribute (though some attributes--- \textit{commitment}/``treatyobligation'' and \textit{mistreatment}/``evil''---seem to form multiple clusters), GPT-2 clusters are less well-formed. This suggests that although GPT-2 produces effect estimates that seem empirically reasonable, we may want to use a different autoregressive language model that has more well-defined clusters. \vl{Or does it really matter, since we're not using GPT-2 language representations, only using it to estimate probabilities?}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{plots/sanity_check/reformer.png}
    \includegraphics[width=0.45\textwidth]{plots/sanity_check/transfo-xl-wt103.png}
    \includegraphics[width=0.45\textwidth]{plots/sanity_check/reformer-finetuned.png}
    \includegraphics[width=0.45\textwidth]{plots/sanity_check/transfo-xl-wt103-finetuned.png}
    \caption{t-SNE plots of sentence embeddings color-coded by the associated text attribute.}
    \label{fig:alias_check2}
\end{figure}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
        \midrule
        Commitment & 71.242 & 68.785 & \textbf{\textcolor{green!50!black}{2.457$^*$}} & [0.024, 4.891] \\
        Bravery & 71.509 & 69.707 & \textcolor{green!50!black}{1.802} & [-1.004, 4.608] \\
        Mistreatment & 70.352 & 70.032 & \textcolor{green!50!black}{0.320} & [-2.516, 3.155] \\
        Flags & 68.436 & 70.544 & \textcolor{red!80!black}{-2.108} & [-5.121, 0.906] \\
        Threat & 68.697 & 70.517 & \textcolor{red!80!black}{-1.820} & [-4.626, 0.986]  \\
        Economy & 68.597 & 70.547 & \textcolor{red!80!black}{-1.950} & [-4.782, 0.882] \\
        Violation & 70.369 & 69.985 & \textcolor{green!50!black}{0.384} & [-2.515, 3.282] \\
        \bottomrule
    \end{tabular}
    \caption{(HK) Treatment effect estimates using the language modeling approach, where text probabilities under $P^T$ are estimated from Reformer fine-tuned with a CLM task on a small number of held-out samples from $P^T$.}
    \label{tab:results_clm_reformer}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
        \midrule
        Commitment & 70.779 & 68.715 & \textcolor{green!50!black}{2.064} & [-0.467, 4.596] \\
        Bravery & 71.041 & 69.275 & \textcolor{green!50!black}{1.766} & [-1.122, 4.654] \\
        Mistreatment & 70.174 & 69.62 & \textcolor{green!50!black}{0.554} & [-2.374, 3.482] \\
        Flags & 68.226 & 70.275 & \textcolor{red!80!black}{-2.049} & [-5.085, 0.987] \\
        Threat & 68.883 & 70.148 & \textcolor{red!80!black}{-1.265} & [-4.099, 1.568]  \\
        Economy & 69.332 & 70.005 & \textcolor{red!80!black}{-0.673} & [-3.509, 2.163] \\
        Violation & 70.166 & 69.63 & \textcolor{green!50!black}{0.536} & [-2.381, 3.453] \\
        \bottomrule
    \end{tabular}
    \caption{(HK) Treatment effect estimates using the language modeling approach, where text probabilities under $P^T$ are estimated from TransformerXL fine-tuned with a CLM task on a small number of held-out samples from $P^T$.}
    \label{tab:results_clm_transformerxl}
\end{table}

t-SNE plots of Reformer \cite{kitaev2020reformer} and TransformerXL \cite{dai2019transformerxl} (Figure \ref{fig:alias_check2}) show better separation by text attribute. We report additional results from the language modeling approach using these models to estimate text probabilities in Tables \ref{tab:results_clm_reformer} and \ref{tab:results_clm_transformerxl}. These estimates are somewhat more conservative that the ones produced by GPT-2 (in that only \textit{commitment} has a significant effect, and even then only for Reformer), and the effect directions are now consistent with the original study. 

\newpage
\appendix

\section{Extra notes}

\vl{These two subsections were generated using ChatGPT and are mostly for my own reference.}

\subsection{Change of measure with Radon-Nikodym derivatives}
\label{sec:change_of_measure}

The Radon-Nikodym derivative can be used to express one probability density function in terms of another probability density function, when the two densities are related by a change of measure. Specifically, if we have two probability measures defined on the same sample space, with one measure $\mathbb{P}$ absolutely continuous with respect to another measure $\mathbb{Q}$, then there exists a Radon-Nikodym derivative $Z$ such that:

$$\mathbb{P}(A) = \int_A Z d\mathbb{Q}$$

for any event $A$ in the sample space. Intuitively, this means that we can define the probability of any event under the measure $\mathbb{P}$ in terms of the probability of the same event under the measure $\mathbb{Q}$, by weighting the probabilities by a factor given by the Radon-Nikodym derivative.

Now, suppose we have two probability density functions $p(x)$ and $q(x)$ defined on some real-valued random variable $X$, with $q(x)>0$ for all $x$. We can interpret $q(x)$ as the "reference" density, and we want to express $p(x)$ in terms of $q(x)$ by a change of measure. To do this, we can define a new probability measure $\mathbb{P}$ as:
$$\mathbb{P}(A) = \int_A \frac{p(x)}{q(x)} q(x) dx = \int_A p(x) dx$$

for any event $A$ in the sample space. This means that we are weighting the probability of each event in proportion to the ratio $p(x)/q(x)$, which is a function of $x$. We can show that $\mathbb{P}$ is absolutely continuous with respect to the measure defined by $q(x)$, and therefore there exists a Radon-Nikodym derivative $Z(x)$ such that:
$$\frac{d\mathbb{P}}{d\mathbb{Q}}(x) = Z(x) = \frac{p(x)}{q(x)}$$

This means that we can express $p(x)$ in terms of $q(x)$ and the Radon-Nikodym derivative $Z(x)$, as:
$$p(x) = Z(x) q(x)$$

This is a general formula for expressing one probability density function in terms of another probability density function by a change of measure.

\subsection{Sentence probabilities from deep language models}
\label{sec:huggingface_sentence_probs}

To get the probability of a sentence from a HuggingFace transformers model, you first need to convert the sentence into a format that the model can understand. Typically, this involves tokenizing the sentence into a sequence of subwords, which is done using the tokenizer that corresponds to the pre-trained model you are using.

Once you have tokenized the sentence, you can pass it to the model to get the probability distribution over the possible output tokens. The probability of the sentence can be computed as the product of the probabilities of each individual token in the sequence.

Here's an example of how to get the probability of a sentence using the HuggingFace Transformers library in Python:

\begin{verbatim}
    from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM
    import torch
    
    # Load the pre-trained model and tokenizer
    model_name = "distilbert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForMaskedLM.from_pretrained(model_name)
    
    # Define the input sentence
    input_sentence = "The cat sat on the mat."
    
    # Tokenize the input sentence
    tokens = tokenizer.encode(input_sentence, return_tensors="pt")
    
    # Get the probability distribution over the tokens
    outputs = model(tokens)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    
    # Compute the probability of the sentence
    probability = 1.0
    for i in range(1, len(tokens[0])):
    probability *= predictions[0, i, tokens[0, i]].item()
    
    print("The probability of the sentence is:", probability)
\end{verbatim}

This code uses the AutoTokenizer and AutoModelForMaskedLM classes from the Transformers library to load a pre-trained model and tokenizer. It then tokenizes the input sentence using the tokenizer, passes the resulting tokens to the model, and computes the probability of the sentence by multiplying together the probabilities of the individual tokens.

\section{Old results}

These results use the old randomization corpus generation method, which did not fully reflect the text generation method used by \cite{fong2021causal}. Additionally, data from the initial study by \cite{fong2021causal} is used, while the current results in the main body of the paper use the results from the replication study.

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
        \midrule
        Commitment & 69.528 & 67.087 & \textcolor{green!50!black}{2.441} & [-7.458, 12.340] \\
        Bravery & 68.384 & 69.487 & \textcolor{red!80!black}{-1.103} & [-12.662, 10.457] \\
        Mistreatment & 69.570 & 68.757 & \textcolor{green!50!black}{0.813} & [-7.262, 8.887] \\
        Flags & 67.572 & 69.264 & \textcolor{red!80!black}{-1.692} & [-11.533, -8.149] \\
        Threat & 67.530 & 69.428 & \textcolor{red!80!black}{-1.898} & [-11.716, -7.922] \\
        Economy & 65.507 & 69.419 & \textcolor{red!80!black}{-3.912} & [-14.135, 6.312] \\
        Violation & 66.729 & 69.255 & \textcolor{red!80!black}{-2.526} & [-11.122, 6.070] \\
        \bottomrule
    \end{tabular}
    \caption{(HK) Treatment effect estimates using the classification approach, where the classifier is a logistic regression model learned over embeddings from pre-trained MPNet.}
    \label{tab:results_clf1_old}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
        \midrule
        Commitment & 64.695 & 65.918 & \textcolor{red!80!black}{-1.223} & [-21.573, 19.127] \\
        Bravery & 63.209 & 68.903 & \textcolor{red!80!black}{-5.694} & [-20.076, 8.688] \\
        Mistreatment & 67.990 & 65.089 & \textcolor{green!50!black}{2.902} & [-9.678, 15.582] \\
        Flags & 60.705 & 68.441 & \textcolor{red!80!black}{-7.737} & [-26.754, 11.280] \\
        Threat & 74.155 & 60.394 & \textcolor{green!50!black}{13.761} & [-4.433, 31.954] \\
        Economy & 59.555 & 65.373 & \textcolor{red!80!black}{-5.818} & [-22.012, 10.376] \\
        Violation & 65.028 & 65.210 & \textcolor{red!80!black}{-0.182} & [-12.003, 11.638] \\
        \bottomrule
    \end{tabular}
    \caption{(HK) Treatment effect estimates using the classification approach, where the classifier is a logistic regression model learned over embeddings from pre-trained MPNet. Text probabilities are computed from marginal sentence probabilities.}
    \label{tab:results_clf2_old}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
        \midrule
        Commitment & 68.377 & 62.907 & \textbf{\textcolor{green!50!black}{5.470}} & [1.944, 8.997] \\
        Bravery & 65.959 & 67.397 & \textcolor{red!80!black}{-1.439} & [-4.621, 1.744] \\
        Mistreatment & 68.266 & 66.625 & \textcolor{green!50!black}{1.641} & [-1.622, 4.904] \\
        Flags & 66.280 & 67.269 & \textcolor{red!80!black}{-0.990} & [-4.175, 2.196] \\
        Threat & 65.864 & 67.397 & \textcolor{red!80!black}{-1.532} & [-4.666, 1.602] \\
        Economy & 66.477 & 67.166 & \textcolor{red!80!black}{-0.689} & [-4.026, 2.649] \\
        Violation & 65.902 & 67.332 & \textcolor{red!80!black}{-1.429} & [-4.691, 1.832] \\
        \bottomrule
    \end{tabular}
    \caption{(HK) Treatment effect estimates using the language modeling approach, where text probabilities under $P^T$ are estimated from GPT-2 fine-tuned with a CLM task on a small number of held-out samples from $P^T$.}
    \label{tab:results_lm2_old}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
        \midrule
        Commitment & 67.694 & 61.536 & \textbf{\textcolor{green!50!black}{6.158}} & [2.212, 10.104] \\
        Bravery & 68.143 & 66.240 & \textcolor{green!50!black}{1.903} & [-1.380, 5.186] \\
        Mistreatment & 67.370 & 66.621 & \textcolor{green!50!black}{0.749} & [-2.642, 4.139] \\
        Flags & 66.935 & 66.812 & \textcolor{green!50!black}{0.123} & [-3.065, 3.311] \\
        Threat & 65.115 & 67.854 & \textcolor{red!80!black}{-2.739} & [-6.073, 0.595] \\
        Economy & 67.197 & 66.742 & \textcolor{green!50!black}{0.455} & [-2.895, 3.806] \\
        Violation & 66.283 & 67.180 & \textcolor{red!80!black}{-0.898} & [-4.169, 2.374] \\
        \bottomrule
    \end{tabular}
    \caption{(HK) Treatment effect estimates using the language modeling approach, where text probabilities under $P^T$ are estimated from pre-trained GPT-2. Text probabilities are computed from marginal sentence probabilities.}
    \label{tab:results_lm3_old}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        Treatment & $\hat{\mu}(P_1)$ & $\hat{\mu}(P_0)$ & $\hat{\tau}$ & 95\% CI \\
        \midrule
        Commitment & 69.254 & 66.250 & \textcolor{green!50!black}{3.004} & [-9.578, 15.586] \\
        Bravery & 68.617 & 68.317 & \textcolor{green!50!black}{0.301} & [-10.940, 11.542] \\
        Mistreatment & 71.439 & 67.742 & \textcolor{green!50!black}{3.696} & [-7.712, 15.103] \\
        Flags & 63.838 & 68.898 & \textcolor{red!80!black}{-5.060} & [-19.985, 9.865] \\
        Threat & 67.750 & 68.870 & \textcolor{red!80!black}{-1.120} & [-12.762, 10.522] \\
        Economy & 64.351 & 69.499 & \textcolor{red!80!black}{-5.148} & [-22.397, 12.101] \\
        Violation & 69.236 & 68.102 & \textcolor{green!50!black}{1.134} & [-10.067, 12.336] \\
        \bottomrule
    \end{tabular}
    \caption{(HK) Treatment effect estimates using the language modeling approach, where text probabilities under $P^T$ are estimated from pre-trained BERT. \vl{This is an improper approach since models trained with MLM, e.g. BERT, can't be used to estimate sentence probabilities directly.}}
    \label{tab:results_lm1_bert}
\end{table}

\newpage
\bibliographystyle{plain}
\bibliography{ref}

\end{document}