\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage[parfill]{parskip}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\newcommand{\vl}[1]{\textcolor{red}{[VL: #1]}}
\allowdisplaybreaks

% blah


\title{Causal Effect Estimation with Text}
\date{}

\begin{document}
	
	\maketitle
	
	\section{Problem setting}
	
	Consider a collection of texts (e.g., documents, sentences, utterances) $\mathcal{X}$, with individual texts $X_i \in \mathcal{X}$.
	\begin{itemize}
		\item Let $X_i = \{g(X_i), h(X_i)\}$, where $g(X_i)$ is the text attribute of interest (i.e., the \textit{treatment}) and $h(X_i)$ denotes all other properties of the text $X_i$. 
		\item Let $Y_i(X_i) = Y_i(g(X_i), h(X_i))$ denote the potential \textit{response} or \textit{outcome} of respondent $i$ after reading text $X_i$.
		\item For simplicity, assume
		\begin{itemize}
			\item $g(X_i) \in \{0, 1\}$ $\forall$ $X_i \in \mathcal{X}$.
			\item Each respondent reads only one text. That is, respondent $i$ reads only $X_i$.
		\end{itemize}
	\end{itemize}
	
	We are interested in knowing the effect of the text attribute $g(X)$ on the response $Y$. Using the standard causal notation, the \textbf{estimand} $\tau^*$ for the effect of $g(X)$ on $Y$ is given by
	\begin{align*}
		\tau^* &= \mathbb{E}_X[Y_i(g(X_i)=1)] - \mathbb{E}_X[Y_i(g(X_i)=0)] \\
		&= \boxed{\sum_{b \in \mathcal{B}} \Big[ \mathbb{E}[Y_i(g(X_i)=1, h(X_i)=b)] - \mathbb{E}[Y_i(g(X_i)=0, h(X_i)=b] \Big]P(h(X_i)=b)}
	\end{align*}
	
	Using stochastic notation, we can write $\tau^*$ more simply.
	\begin{itemize}
		\item Let $P_{1 g,h}(X)$ denote a distribution such that $g(X)=1$ and $h(X) \sim P^*$, where $P^*$ is some arbitrary probability distribution.
		\item Let $P_{0 g,h}(X)$ denote a distribution such that $g(X)=0$ and $h(X) \sim P^*$.
		\item Let the quantity $\mu(P)$ be defined as follows:
		\begin{align*}
			\mu(P) &= E_{X_i \sim P_{g,h}}[Y_i(X_i)] \\
			&= \frac{1}{N} \sum_{i=1}^N Y_i(X_i)P_{g,h}(X_i)
		\end{align*}
	\end{itemize}
	
	This allows us to express $\tau^*$ as
	\begin{equation*}
		\boxed{\tau^* = \mu(P_1)-\mu(P_0)}
	\end{equation*}
	
	\section{Estimator}
	
	\subsection{Data setting}
	
	Suppose we have some data collected from a randomized trial, in which subjects $i \in [N]$ are shown various texts randomized over $g(X)$ and $h(X)$. These texts are constructed from individual components in a generative way. 
	\begin{itemize}
		\item That is, to construct a text where $g(X)=1$, the text may include one of several selected sentences that are chosen to correspond to $g(X)=1$.
		\item Likewise, for various attributes comprising $h(X)=(h_1(X), h_2(X), h_3(X), \dots)$, there may be several candidate texts that correspond to $h_1(X)=1$, several candidate texts that correspond to $h_2(X)=0$, and so on.
	\end{itemize}
	
	While we are guaranteed to be able to obtain an unbiased effect estimate for $g(X)$ from this trial (e.g., using the plug-in estimator), the estimate corresponds only to the effect of $g(X)$ \textit{in this specific text setting}. The type of text setting constructed in this trial is fairly artificial, and we believe that the effect may be different depending on the text setting. It is important, for example, to know if this effect still holds in natural text settings.
	
	Therefore, we are interested in knowing the effect of $g(X)$ under text distributions that are different from the initial text distribution. Again, some notation:
	
	\begin{itemize}
		\item Let $P^R_{g,h} = P^R$ denote the \textit{randomization distribution}, or the distribution of texts as constructed in the randomized trial.
		\item Let $P^T_{g,h} = P^T$ denote the \textit{target distribution}, or the text distribution of interest.
		\item Let $\mathbb{P}(A) = \int_A P(X) dx$.
		\item Using the Radon-Nikodym derivative for change of measure (see Section \ref{sec:change_of_measure}), we can write $P^T$ in terms of $P^R$ as
		\begin{equation*}
			P^T(X) = \frac{d\mathbb{P}^T}{d\mathbb{P}^R}(X)P^R(X)
		\end{equation*}
		\begin{itemize}
			\item We require the \href{https://en.wikipedia.org/wiki/Absolute_continuity#Absolute_continuity_of_measures}{absolute continuity} of $\mathbb{P}^T$ with respect to  $\mathbb{P}^R$, such that $P^R(X) = 0 \Rightarrow P^R(X) = 0$.
		\end{itemize}
		\item Now, let $(X_i,Y_i(X_i))_{i=1}^n \sim P^R$ correspond to \textit{observed} pairs of features and outcomes sampled from the randomization distribution $P^R$.
	\end{itemize}
	
	\subsection{Horvitz-Thompson estimator}
	
	To define our effect estimate in terms of our target distribution $P^T$, we propose using the ratio between $P^T$ and $P^R$ as importance weights in an Horvitz-Thompson estimator. Using the change of measure for $P^T(X)$ we defined previously, this gives us 
	% \vl{Do we need to normalize by $\frac{1}{\sum_{i=1}^n \hat{P}^T(X_i)}$ in the first line?}
	\begin{align*}
		\hat{\mu}(P^T) &= \frac{1}{n} \sum_{i=1}^n \frac{\hat{P}^T(X_i)}{P^R(X_i)}Y_i(X_i) \\
		&= \frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)\frac{P^R(X_i)}{P^R(X_i)} Y_i(X_i) \\
		&=\boxed{\frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)}
	\end{align*}
	
	We can show that $\hat{\mu}(P^T)$ is an unbiased estimator for $\mu(P^T)$:
	\begin{align*}
		\mathbb{E}[\hat{\mu}(P)] &= \mathbb{E}_{X \sim P^R}[\hat{\mu}(P)] \\
		&= \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\right] \\
		&=\frac{1}{n}\sum_{i=1}^n \mathbb{E}_{X \sim P^R}\left[\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\right] \\
		&= \mathbb{E}_{X \sim P^R}\left[\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\right] \\
		&=\mathbb{E}_{X \sim P^T}[Y_i(X_i)] \\
		&= \mu(P^T)
	\end{align*}
	
	Finally, letting $\mathcal{X}$ be the space of all texts, the variance of the estimator is given by:
	\begin{align*}
		\text{Var}[\hat{\mu}(P)] &= \text{Var}_X\left[\frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\right] \\
		&= \text{Cov}_X\left[\frac{1}{n} \sum_{i=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i), \frac{1}{n} \sum_{j=1}^n \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j(X_j)\right] \\
		&= \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \text{Cov}_X\left[\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i), \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j(X_j)\right] \\
		&= \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \text{Cov}_X\left[\sum_{x \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\mathbbm{1}\{X_i = x\}, \sum_{x' \in \mathcal{X}}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j(X_j)\mathbbm{1}\{X_j=x'\}\right] \\
		&= \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n \text{Cov}_X\left[\int_x\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)Y_i(X_i)\mathbbm{1}\{X_i = x\}d\mathbb{P}^R(x), \int_{x'}\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j(X_j)\mathbbm{1}\{X_j=x'\}d\mathbb{P}^R(x')\right] \\
		&= \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n \int_x \int_{x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(X_i)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(X_j)Y_i(X_i)Y_j(X_j) \text{Cov})_X[\mathbbm{1}\{X_i=x\}, \mathbbm{1}\{X_i=x'\}]d\mathbb{P}^R(x)d\mathbb{P}^R(x') \\
		&= \frac{1}{n^2} \sum_{i,j \in [n]} \iint_{x, x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(X_i)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(X_j)Y_i(X_i)Y_j(X_j) \text{Cov}_X[\mathbbm{1}\{X_i=x\}, \mathbbm{1}\{X_i=x'\}]d\mathbb{P}^R(x)d\mathbb{P}^R(x') \\
		&= \frac{1}{n^2} \sum_{i,j \in [n]} \iint_{x, x'} \frac{d \mathbb{P}^T}{d \mathbb{P}^R}(X_i)\frac{d \mathbb{P}^T}{d \mathbb{P}^R}(X_j)Y_i(X_i)Y_j(X_j) (P^R(X_i,X_j) - P^R(X_i)P^R(X_j))d\mathbb{P}^R(x)d\mathbb{P}^R(x')
	\end{align*}
	
	where the last equality follows from
	\begin{align*}
		\text{Cov}[\mathbbm{1}\{X_i=x\},\mathbbm{1}\{X_j=x'\}] &=\mathbb{E}_X[\mathbbm{1}\{X_i=x\}\mathbbm{1}\{X_j=x'\}] - \mathbb{E}_X[\mathbbm{1}\{X_i=x\}]\mathbb{E}_X[\mathbbm{1}\{X_j=x'\}] \\
		&= P^R(X_i,X_j) - P^R(X_i)P^R(X_j)
	\end{align*}
	
	With the central limit theorem (CLT), we establish asymptotic normality:
	\begin{equation*}
		\frac{\hat{\mu}(P) - \mu(P)}{\sqrt{\text{Var}[\hat{\mu}(P)]}}\rightarrow N(0,1)
	\end{equation*}
	
	which we can use to estimate confidence intervals using the following unbiased estimate for the variance.
	\begin{equation*}
		\widehat{\text{Var}}[\hat{\mu}(P)] = \frac{1}{n^2} \sum_{i,j \in [n]} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_i(X_i)Y_j(X_j)\frac{P^R(X_i,X_j) - P^R(X_i)P^R(X_j)}{P^R(X_i,X_j)}
	\end{equation*}
	
	\vl{Do we compute variance estimates separately for $\hat{\mu}(P_1)$ and $\hat{\mu}(P_1)$, then compute $$\widehat{\text{Var}}(\hat{\tau})=\sqrt{\frac{1}{nP(g(X)=1)}\sqrt{\widehat{\text{Var}}[\hat{\mu}(P_1)} + \frac{1}{nP(g(X)=0)}\sqrt{\widehat{\text{Var}}[\hat{\mu}(P_0)}}$$ Or do we use this variance estimate over the whole sample to directly get $\widehat{\text{Var}}(\hat{\tau})$?}
	
	\section{Empirical estimation}
	\subsection{Estimating $\hat{P}^T$ from the data}
	
	\vl{Should we ever use the full randomization corpus (i.e., all possible 2- or 3-attribute permutations of candidate sentences from the HK study) as $P^R$, or do we always consider $P^R$ to be the sample (i.e., the data for which we have measured outcomes)? This is relevant because the distributions are not the same---the reason being that there are different numbers of candidate sentences for each text attribute, so a full set of permutations is not uniform over the confounding attributes.}
	
	\vl{As a related question, should the distribution of number of sentences per text in $P^T$ follow sample $P^R$ or full-corpus $P^R$?}
	
	Our estimator has one primary nuisance parameter, $\frac{\hat{d\mathbb{P}^T}(X)}{d\mathbb{P}^R(X)}$ (or $\hat{P}^T(X)$, depending on the version of the estimator we want to use). We propose several approaches for estimating this quantity.
	
	\vl{Is sample splitting necessary when learning the nuisance parameter?}
	
	\subsubsection{Classification}
	\label{sec:classification}
	
	We notice that we can rewrite $\frac{d\mathbb{P}^T}{d\mathbb{P}^R}$. Let $C$ denote the distribution (or corpus) from which a text is drawn, where $C=T$ denotes that it is drawn from $P^T$ and $C=R$ denotes that it is drawn from $P^R$. Then
	\begin{equation*}
		\begin{split}
			&\frac{d\mathbb{P}^T}{d\mathbb{P}^R}(X) = \frac{P(C=T|X)}{P(C=T)}\frac{P(C=R)}{P(C=R|X)}\\
			\Rightarrow &\frac{\hat{d\mathbb{P}^T}}{d\mathbb{P}^R}(X) = \frac{\hat{P}(C=T|X)}{P(C=T)}\frac{P(C=R)}{\hat{P}(C=R|X)}
		\end{split}
	\end{equation*}
	
	where estimation of $\hat{P}(C=T|X)$ and $\hat{P}(C=R|X)$ is straightforward---we can train a binary classifier $M_\theta: \mathcal{X} \rightarrow \{0,1\}$ to predict if a text $X$ came from $T$ or $R$---and $P(C=R)$ and $P(C=T)$ are their sample proportions over the entire body of text.
	
	We have several options for our binary classifier:
	\begin{itemize}
		\item A model that takes ``interpretable'' language features as input (e.g., bag-of-words, lexicon, SenteCon). We train a ``simple'' classifier over these features (e.g., logistic regression, SVM).
		\item A deep language model that takes raw text as input. We can either use a pre-trained model, or we can fine-tune the pre-trained model to differentiate between text examples from $P^T$ and text examples from $P^R$.
		\item \vl{Since texts from $P^R$ are artificial, the classification task might be very easy, so $\hat{P}(C=T|X)$ and $\hat{P}(C=R|X)$ may be very close to either 0 or 1. Instead of training a discriminative classifier, we could have two generative models, one for $P^R$ and one for $P^T$. Then given an $X$, which model is more likely? This could help with the overconfidence of the discriminative classifier.}
	\end{itemize}
	
	\subsubsection{Language tasks}
	
	Language models like transformers are capable of directly computing the probability of a sentence (see Section \ref{sec:huggingface_sentence_probs}), even if the model has never seen the sentence before. This probability is based on the text distribution used to train the model. Since the state-of-the-art language models are pre-trained on extremely large corpora that approximate the entirety of the English language, we must further fine-tune before we can use them to compute sentence probabilities for $P^T$.
	
	Several tasks that may be useful for encouraging a language model $M_\theta$ to learn $P^T$:
	\begin{itemize}
		\item We draw text examples from $P^T$. For each sentence, certain words are masked, and $M_\theta$ learns to predict them.
		\item Causal language modeling (CLM)/text generation: Again, we drawn text samples from $P^T$. For each sentence, starting from the first word, $M_\theta$ learns to predict the next word.
		\item Style transfer: Given a text example drawn from $P^R$, $M_\theta$ reproduces that same text example ``in the style of'' $P^T$. This task requires $M_\theta$ to be a sequence-to-sequence language model, like \href{https://arxiv.org/pdf/1910.13461.pdf}{BART}, and for best results, parallel text examples from $P^R$ and $P^T$ should be available.
	\end{itemize}
	
	For a practical example of MLM and CLM, see \href{https://huggingface.co/docs/transformers/tasks/language_modeling}{this example from HuggingFace}. For some ideas on how to fine-tune BART on a style transfer task, see \href{https://blog.fastforwardlabs.com/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html}{this blog post} and \href{https://arxiv.org/pdf/2105.06947.pdf}{this paper}.
	
	\vl{Because texts from $P^R$ are very artificial (especially the sequences of sentences), it is possible that $P^T$ of those texts will be very low, perhaps more than they should be. $P^T$ of individual sentences may not be that low, but the overall construction will have low probability. Is this a problem? Maybe we should compute $P^T(\text{sentence 1})P^T(\text{sentence 2})P^T(\text{sentence 3})$ instead of $P^T(\text{sentence 1, sentence 2, sentence 3})$...}
	
	\vl{If you fine-tune an LM, how do you guarantee that it's learned enough of $P^T$ and forgotten enough of its original training domain? Also, how do you guarantee that it moves away from $P^R$?}
	
	\subsection{Estimating $\hat{\mu}(P)$ from the data}
	
	We estimate the overall quantity of interest $\hat{\tau}$ through the following procedure. Suppose we have samples $(X_i,Y_i)_{i=1}^n \sim P^R$. In our experiments, these samples are from this \href{https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12649}{2021 Fong and Grimmer paper}. The authors conduct a randomized study on the effects of a text attribute $g(X)$ on an outcome $Y$ in a constructed text setting, which we consider to be distributed $P^R$.
	
	\begin{enumerate}
		\item Sample examples from $P^T$. Train $M_\theta$ on these examples to be able to obtain $\frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_i)$ or $\hat{P}^T(X_i)$ $\forall$ $i \in [n]$.
		\begin{itemize}
			\item[--] In our experiments, these examples from $P^T$ constitute 2-7 sentence chunks drawn from the U.S. Congressional speeches on the Hong Kong protests. We consider these to be the ``natural language counterparts'' of the examples from $P^R$.
		\end{itemize}
		\item (If using $P^T$) $P^R(X_i)$ should be known from the design of the randomized study. In the worst case, it can be computed empirically from sample proportions.
		\item Split $(X_i,Y_i)$ according to samples where $g(X_i)=1$ and where $g(X_i)=0$. Call these data splits $D_1$ and $D_0$, respectively.
		\item Compute
		\begin{equation*}
			\begin{split}
				\hat{\tau} &= \hat{\mu}(P_1) - \hat{\mu}(P_0) \\
				&= \frac{1}{nP(g(X)=1)} \sum_{j \in D_1} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_j)Y_j - \frac{1}{nP(g(X)=0)} \sum_{k \in D_0} \frac{\hat{d \mathbb{P}^T}}{d \mathbb{P}^R}(X_k)Y_k \\
				&= \frac{1}{nP(g(X)=1)} \sum_{j \in D_1} \frac{\hat{P}(C=T|X_j)}{P(C=T)}\frac{P(C=R)}{\hat{P}(C=R|X_j)}Y_j - \frac{1}{nP(g(X)=0)} \sum_{k \in D_0} \frac{\hat{P}(C=T|X_k)}{P(C=T)}\frac{P(C=R)}{\hat{P}(C=R|X_k)}Y_k
			\end{split}
		\end{equation*}
		or alternatively (to ensure that the absolute continuity condition holds, we restrict $P^T$ with respect to the support of the randomization distribution, such that $P^T(X) = \frac{P^T(\tilde{X})\mathbbm{1}\{\tilde{X} \in C^R\}}{\sum_{i=1}^n P^T(X_i)}$)
		\begin{equation*}
			\begin{split}
				\hat{\tau} &= \hat{\mu}(P_1) - \hat{\mu}(P_0) \\
				&= \frac{1}{n P(g(X)=1)} \sum_{j \in D_1} \frac{\hat{P}^T(X_j)}{P^R(X_j)\sum_{i=1}^n \hat{P}^T(X_i)}Y_j - \frac{1}{nP(g(X)=0)} \sum_{k \in D_0} \frac{\hat{P}^T(X_k)}{P^R(X_k)\sum_{i=1}^n \hat{P}^T(X_i)}Y_k
			\end{split}
		\end{equation*}
		% \vl{Is the normalization just to ensure that $\hat{P}^T$ sums to 1 over $X_i \sim P^R$?}
		
		Note that $P(g(X)=1)$ and $P(g(X)=0)$ are defined over the entire randomization corpus (i.e., every possible permutation of the statements/linguistic attributes in the HK study), prior to sampling. In practical terms, this means they are computed from the  proportions of the randomization corpus, rather than the  proportions of the sampled data (i.e., the data for which we have measured outcomes).
	\end{enumerate}
	
	\section{Extra notes}
	
	\vl{These two subsections were generated using ChatGPT and are mostly for my own reference.}
	
	\subsection{Change of measure with Radon-Nikodym derivatives}
	\label{sec:change_of_measure}
	
	The Radon-Nikodym derivative can be used to express one probability density function in terms of another probability density function, when the two densities are related by a change of measure. Specifically, if we have two probability measures defined on the same sample space, with one measure $\mathbb{P}$ absolutely continuous with respect to another measure $\mathbb{Q}$, then there exists a Radon-Nikodym derivative $Z$ such that:
	
	$$\mathbb{P}(A) = \int_A Z d\mathbb{Q}$$
	
	for any event $A$ in the sample space. Intuitively, this means that we can define the probability of any event under the measure $\mathbb{P}$ in terms of the probability of the same event under the measure $\mathbb{Q}$, by weighting the probabilities by a factor given by the Radon-Nikodym derivative.
	
	Now, suppose we have two probability density functions $p(x)$ and $q(x)$ defined on some real-valued random variable $X$, with $q(x)>0$ for all $x$. We can interpret $q(x)$ as the "reference" density, and we want to express $p(x)$ in terms of $q(x)$ by a change of measure. To do this, we can define a new probability measure $\mathbb{P}$ as:
	$$\mathbb{P}(A) = \int_A \frac{p(x)}{q(x)} q(x) dx = \int_A p(x) dx$$
	
	for any event $A$ in the sample space. This means that we are weighting the probability of each event in proportion to the ratio $p(x)/q(x)$, which is a function of $x$. We can show that $\mathbb{P}$ is absolutely continuous with respect to the measure defined by $q(x)$, and therefore there exists a Radon-Nikodym derivative $Z(x)$ such that:
	$$\frac{d\mathbb{P}}{d\mathbb{Q}}(x) = Z(x) = \frac{p(x)}{q(x)}$$
	
	This means that we can express $p(x)$ in terms of $q(x)$ and the Radon-Nikodym derivative $Z(x)$, as:
	$$p(x) = Z(x) q(x)$$
	
	This is a general formula for expressing one probability density function in terms of another probability density function by a change of measure.
	
	\subsection{Sentence probabilities from deep language models}
	\label{sec:huggingface_sentence_probs}
	
	To get the probability of a sentence from a HuggingFace transformers model, you first need to convert the sentence into a format that the model can understand. Typically, this involves tokenizing the sentence into a sequence of subwords, which is done using the tokenizer that corresponds to the pre-trained model you are using.
	
	Once you have tokenized the sentence, you can pass it to the model to get the probability distribution over the possible output tokens. The probability of the sentence can be computed as the product of the probabilities of each individual token in the sequence.
	
	Here's an example of how to get the probability of a sentence using the HuggingFace Transformers library in Python:
	
	\begin{verbatim}
		from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM
		import torch
		
		# Load the pre-trained model and tokenizer
		model_name = "distilbert-base-uncased"
		tokenizer = AutoTokenizer.from_pretrained(model_name)
		model = AutoModelForMaskedLM.from_pretrained(model_name)
		
		# Define the input sentence
		input_sentence = "The cat sat on the mat."
		
		# Tokenize the input sentence
		tokens = tokenizer.encode(input_sentence, return_tensors="pt")
		
		# Get the probability distribution over the tokens
		outputs = model(tokens)
		predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
		
		# Compute the probability of the sentence
		probability = 1.0
		for i in range(1, len(tokens[0])):
		probability *= predictions[0, i, tokens[0, i]].item()
		
		print("The probability of the sentence is:", probability)
	\end{verbatim}
	
	This code uses the AutoTokenizer and AutoModelForMaskedLM classes from the Transformers library to load a pre-trained model and tokenizer. It then tokenizes the input sentence using the tokenizer, passes the resulting tokens to the model, and computes the probability of the sentence by multiplying together the probabilities of the individual tokens.
	
\end{document}
